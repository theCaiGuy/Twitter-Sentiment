{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split - deprecated\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_data_path = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/training-full.csv'\n",
    "partial_data_path = './data/training-full.csv'\n",
    "\n",
    "total_size = sum(1 for line in open(partial_data_path, encoding = 'latin-1')) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini training data set of 10000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Pos_Neg          ID                          Date     QUERY  \\\n",
      "0           0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1           0  1467894600  Mon Apr 06 22:41:51 PDT 2009  NO_QUERY   \n",
      "2           0  1467972262  Mon Apr 06 23:03:39 PDT 2009  NO_QUERY   \n",
      "3           0  1468047066  Mon Apr 06 23:26:06 PDT 2009  NO_QUERY   \n",
      "4           0  1468121466  Mon Apr 06 23:49:56 PDT 2009  NO_QUERY   \n",
      "5           0  1468192521  Tue Apr 07 00:13:02 PDT 2009  NO_QUERY   \n",
      "6           0  1468262100  Tue Apr 07 00:36:46 PDT 2009  NO_QUERY   \n",
      "7           0  1468336500  Tue Apr 07 01:03:03 PDT 2009  NO_QUERY   \n",
      "8           0  1468401268  Tue Apr 07 01:26:36 PDT 2009  NO_QUERY   \n",
      "9           0  1468470368  Tue Apr 07 01:52:04 PDT 2009  NO_QUERY   \n",
      "10          0  1468543053  Tue Apr 07 02:18:49 PDT 2009  NO_QUERY   \n",
      "11          0  1468617899  Tue Apr 07 02:45:41 PDT 2009  NO_QUERY   \n",
      "12          0  1468686594  Tue Apr 07 03:09:58 PDT 2009  NO_QUERY   \n",
      "13          0  1468755779  Tue Apr 07 03:33:42 PDT 2009  NO_QUERY   \n",
      "14          0  1468833927  Tue Apr 07 03:58:41 PDT 2009  NO_QUERY   \n",
      "15          0  1468913518  Tue Apr 07 04:22:39 PDT 2009  NO_QUERY   \n",
      "16          0  1468998615  Tue Apr 07 04:46:29 PDT 2009  NO_QUERY   \n",
      "17          0  1469085298  Tue Apr 07 05:08:30 PDT 2009  NO_QUERY   \n",
      "18          0  1469176829  Tue Apr 07 05:29:55 PDT 2009  NO_QUERY   \n",
      "19          0  1469289352  Tue Apr 07 05:54:00 PDT 2009  NO_QUERY   \n",
      "20          0  1469414844  Tue Apr 07 06:18:34 PDT 2009  NO_QUERY   \n",
      "21          0  1469529446  Tue Apr 07 06:39:30 PDT 2009  NO_QUERY   \n",
      "22          0  1469663416  Tue Apr 07 07:03:12 PDT 2009  NO_QUERY   \n",
      "23          0  1469784713  Tue Apr 07 07:24:42 PDT 2009  NO_QUERY   \n",
      "24          0  1469901491  Tue Apr 07 07:45:24 PDT 2009  NO_QUERY   \n",
      "25          0  1470036251  Tue Apr 07 08:09:22 PDT 2009  NO_QUERY   \n",
      "26          0  1470152257  Tue Apr 07 08:30:14 PDT 2009  NO_QUERY   \n",
      "27          0  1548286554  Fri Apr 17 20:32:27 PDT 2009  NO_QUERY   \n",
      "28          0  1548551581  Fri Apr 17 21:16:15 PDT 2009  NO_QUERY   \n",
      "29          0  1548688822  Fri Apr 17 21:40:15 PDT 2009  NO_QUERY   \n",
      "...       ...         ...                           ...       ...   \n",
      "4971        4  2191449563  Tue Jun 16 05:17:59 PDT 2009  NO_QUERY   \n",
      "4972        4  2191498786  Tue Jun 16 05:24:11 PDT 2009  NO_QUERY   \n",
      "4973        4  2191549437  Tue Jun 16 05:30:23 PDT 2009  NO_QUERY   \n",
      "4974        4  2191600931  Tue Jun 16 05:36:37 PDT 2009  NO_QUERY   \n",
      "4975        4  2191665733  Tue Jun 16 05:44:17 PDT 2009  NO_QUERY   \n",
      "4976        4  2191715430  Tue Jun 16 05:50:08 PDT 2009  NO_QUERY   \n",
      "4977        4  2191787040  Tue Jun 16 05:58:15 PDT 2009  NO_QUERY   \n",
      "4978        4  2191845446  Tue Jun 16 06:04:23 PDT 2009  NO_QUERY   \n",
      "4979        4  2191904422  Tue Jun 16 06:10:30 PDT 2009  NO_QUERY   \n",
      "4980        4  2191981070  Tue Jun 16 06:18:26 PDT 2009  NO_QUERY   \n",
      "4981        4  2192041073  Tue Jun 16 06:24:35 PDT 2009  NO_QUERY   \n",
      "4982        4  2192103859  Tue Jun 16 06:30:51 PDT 2009  NO_QUERY   \n",
      "4983        4  2192182835  Tue Jun 16 06:38:37 PDT 2009  NO_QUERY   \n",
      "4984        4  2192249130  Tue Jun 16 06:44:53 PDT 2009  NO_QUERY   \n",
      "4985        4  2192315621  Tue Jun 16 06:51:01 PDT 2009  NO_QUERY   \n",
      "4986        4  2192397932  Tue Jun 16 06:58:47 PDT 2009  NO_QUERY   \n",
      "4987        4  2192514568  Tue Jun 16 07:09:03 PDT 2009  NO_QUERY   \n",
      "4988        4  2192584392  Tue Jun 16 07:15:13 PDT 2009  NO_QUERY   \n",
      "4989        4  2192673182  Tue Jun 16 07:23:05 PDT 2009  NO_QUERY   \n",
      "4990        4  2192768974  Tue Jun 16 07:31:22 PDT 2009  NO_QUERY   \n",
      "4991        4  2192839605  Tue Jun 16 07:37:33 PDT 2009  NO_QUERY   \n",
      "4992        4  2192935901  Tue Jun 16 07:45:55 PDT 2009  NO_QUERY   \n",
      "4993        4  2193007920  Tue Jun 16 07:52:05 PDT 2009  NO_QUERY   \n",
      "4994        4  2193082128  Tue Jun 16 07:58:23 PDT 2009  NO_QUERY   \n",
      "4995        4  2193179229  Tue Jun 16 08:06:14 PDT 2009  NO_QUERY   \n",
      "4996        4  2193254134  Tue Jun 16 08:12:22 PDT 2009  NO_QUERY   \n",
      "4997        4  2193322665  Tue Jun 16 08:18:02 PDT 2009  NO_QUERY   \n",
      "4998        4  2193427329  Tue Jun 16 08:26:39 PDT 2009  NO_QUERY   \n",
      "4999        4  2193503480  Tue Jun 16 08:32:48 PDT 2009  NO_QUERY   \n",
      "5000        4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                 User                                            Content  \n",
      "0     _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1              dreaaa  throat is closing up and i had some string che...  \n",
      "2       Smith_Cameron  @hillary006 I'm sure everyone has ruined my gi...  \n",
      "3           dianapwns           @alexbigman you left without saying hi!   \n",
      "4         tominlumban  Yo jimo i cant talk on aim anymore, its glitch...  \n",
      "5      catherinestack                      @laurenlenewx awww i'm sorry   \n",
      "6             Fatty_D  @KellyShibari i thought i saw you there! you w...  \n",
      "7          bethasaurr  The one day i really need to go into school an...  \n",
      "8     soapdishsailing  I'm having a panic attack, so I can't sleep. D...  \n",
      "9             jj_tins                                    Deadline ahead   \n",
      "10           alunjohn  @Claire_S Will you be videoing or streaming or...  \n",
      "11        jaredgunter        Up since 3:00. Going to be a looooong day.   \n",
      "12      DivasMistress  @nikkiwoods Exactamundo!!! For some reason I t...  \n",
      "13          rpecknold  Going deaf in my right ear. Too many feedback ...  \n",
      "14      swimmingfishy  Going to school and enjoying my last day as a ...  \n",
      "15        Roonaldo107              @Tracy_R Evil!! I have a prawn salad   \n",
      "16          Poppypaws                         wishes things were easier   \n",
      "17      morganbrennan               I feel like DEATH.  My throat hurts.  \n",
      "18          tongirl02  House completly surprised me last night. I can...  \n",
      "19           Ciridian  would have liked to have gotten more sleep las...  \n",
      "20              skoop  I still dearly miss the symfony dev environmen...  \n",
      "21     IPanicAtDiscos   dentist later, I'm hoping it's my teeth plate...  \n",
      "22           amysav83                        @jeayese coz its cold rain   \n",
      "23       steffy_weffy                            @dinadb where are you?   \n",
      "24         auntyadele           @vix7 I swear I'm going to unfollow you   \n",
      "25          lindsaycb  @enoch111 Whoops. I got a little too happy. Do...  \n",
      "26             BenZee  @ericahoff  I'm sorry to hear that Erica... bu...  \n",
      "27    Flyinwatermelon  @lillyputian whats gotten into me these few da...  \n",
      "28         photokitty  Freakin' crap! I just bit my tongue on accident.   \n",
      "29        ubringmejoi                                   my nose is cold   \n",
      "...               ...                                                ...  \n",
      "4971         Aaronage  @williamtm &gt; treat us well over the years s...  \n",
      "4972          swizzem   uploadin my jonas pics  ill put some on in a bit  \n",
      "4973       Taoteaking  @sticksngiggles Perhaps your right ... but ......  \n",
      "4974            BoL7z  @Ninja_Catfish my canalphones went through the...  \n",
      "4975       Kawaii_Emi  @liveeisavampire http://twitpic.com/7iq0a - ni...  \n",
      "4976    FitnessFoodie  it's a beautiful morning and I had a creamy gl...  \n",
      "4977          Jenidvm  And if he thinks that cooing sweetly at me wil...  \n",
      "4978       AlyxxDione  Oh yeaa.... #squarespace i love you... forever...  \n",
      "4979    vandawilliams  is excited to make a new video. its been a while   \n",
      "4980        Clyde_DTH          make beatiful pics with my new Nikon D90   \n",
      "4981     minhtrangcat                 KEB - the very best bank in Korea   \n",
      "4982         xxbonnie  why am i up so early? well, at least my mommy ...  \n",
      "4983       thepaulkim  drinking blueberry green tea   oh and for thos...  \n",
      "4984       cindergela        @SaintJboyd hoy! i greeted you last night!   \n",
      "4985     pooping24998  Are you looking for a good value affiliate pro...  \n",
      "4986      ktkeroscene                         @dannykurily welcome home   \n",
      "4987         _alover_  @Rachael90210 day's going great thanx  course ...  \n",
      "4988           shae75                  @Foxy_HotSawce good morning love   \n",
      "4989      1ChoSenOne8  This Morning I woke up feeling like money, I j...  \n",
      "4990              snh  @rachbarnhart I have plenty of opinions that I...  \n",
      "4991         Bthnycks  @hobnobsftww_  oo paramore are bringing out a ...  \n",
      "4992        MikeTreat          @PreciousGemGem thanks! will investigate   \n",
      "4993       bofranklin  Christ, it's sticky today... Taking a stroll h...  \n",
      "4994      pshhitscaty  @Littlehotrod  yeah its just me ill be in play...  \n",
      "4995             jmt1           hey it's Jeff  http://aweber.com/b/1huHV  \n",
      "4996      dennis_luis  @mikasounds if you like to support talented yo...  \n",
      "4997   diegolikecrazy  @MariaLKanellis true, true, also gets you into...  \n",
      "4998        whipzilla  - had a great time with some of the best peopl...  \n",
      "4999         JConnell  @theokk don't know what you could possibly mea...  \n",
      "5000   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[5001 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "train_n = 1\n",
    "train_skip = [x for x in range(1, total_size) if x % train_n != 0]\n",
    "train_data = pd.read_csv(full_data_path, skiprows=train_skip, encoding = 'latin-1', names = [\"Pos_Neg\", \"ID\", \"Date\", \"QUERY\", \"User\", \"Content\"])\n",
    "print (train_data)\n",
    "train_data.to_csv('train_full.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini dev data set of 500 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_n = 1600\n",
    "dev_skip = [x for x in range(0, total_size) if (x + 10) % dev_n != 0]\n",
    "#print (dev_skip)\n",
    "dev_data = pd.read_csv(full_data_path, skiprows=dev_skip, encoding = 'latin-1', names = [\"Pos_Neg\", \"ID\", \"Date\", \"QUERY\", \"User\", \"Content\"])\n",
    "#print (dev_data)\n",
    "\n",
    "for i in range(dev_data.shape[0]):\n",
    "    if (train_data['Content'] == dev_data.loc[i]['Content']).any():\n",
    "        dev_data = dev_data.drop([i], axis=0)\n",
    "        \n",
    "dev_data.to_csv('dev_mini.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini test data set of 500 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n = 3200\n",
    "test_skip = [x for x in range(0, total_size) if (x + 15) % test_n != 0]\n",
    "test_data = pd.read_csv(full_data_path, skiprows=test_skip, encoding = 'latin-1', names = [\"Pos_Neg\", \"ID\", \"Date\", \"QUERY\", \"User\", \"Content\"])\n",
    "# print (test_data)\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    if (train_data['Content'] == test_data.loc[i]['Content']).any():\n",
    "        test_data = test_data.drop([i], axis=0)\n",
    "    if train_data['Pos_Neg'] == 2:\n",
    "        print (\"Look!\")\n",
    "\n",
    "#test_data.to_csv('test_mini.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
