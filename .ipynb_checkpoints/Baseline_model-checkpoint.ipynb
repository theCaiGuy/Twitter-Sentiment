{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  Pos_Neg          ID                          Date  \\\n",
      "0               0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
      "1               1        0  1467823851  Mon Apr 06 22:23:09 PDT 2009   \n",
      "2               2        0  1467841832  Mon Apr 06 22:27:55 PDT 2009   \n",
      "3               3        0  1467859820  Mon Apr 06 22:32:36 PDT 2009   \n",
      "4               4        0  1467876652  Mon Apr 06 22:37:03 PDT 2009   \n",
      "5               5        0  1467894600  Mon Apr 06 22:41:51 PDT 2009   \n",
      "6               6        0  1467909222  Mon Apr 06 22:45:53 PDT 2009   \n",
      "7               7        0  1467926632  Mon Apr 06 22:50:51 PDT 2009   \n",
      "8               8        0  1467943526  Mon Apr 06 22:55:40 PDT 2009   \n",
      "9               9        0  1467953733  Mon Apr 06 22:58:37 PDT 2009   \n",
      "10             10        0  1467972262  Mon Apr 06 23:03:39 PDT 2009   \n",
      "11             11        0  1467986889  Mon Apr 06 23:07:54 PDT 2009   \n",
      "12             12        0  1468000948  Mon Apr 06 23:11:55 PDT 2009   \n",
      "13             13        0  1468017259  Mon Apr 06 23:16:45 PDT 2009   \n",
      "14             14        0  1468033983  Mon Apr 06 23:21:57 PDT 2009   \n",
      "15             15        0  1468047066  Mon Apr 06 23:26:06 PDT 2009   \n",
      "16             16        0  1468063614  Mon Apr 06 23:31:06 PDT 2009   \n",
      "17             17        0  1468077666  Mon Apr 06 23:35:28 PDT 2009   \n",
      "18             18        0  1468097006  Mon Apr 06 23:41:49 PDT 2009   \n",
      "19             19        0  1468108912  Mon Apr 06 23:45:43 PDT 2009   \n",
      "20             20        0  1468121466  Mon Apr 06 23:49:56 PDT 2009   \n",
      "21             21        0  1468133212  Mon Apr 06 23:53:53 PDT 2009   \n",
      "22             22        0  1468147058  Mon Apr 06 23:58:34 PDT 2009   \n",
      "23             23        0  1468161033  Tue Apr 07 00:02:57 PDT 2009   \n",
      "24             24        0  1468174290  Tue Apr 07 00:07:08 PDT 2009   \n",
      "25             25        0  1468192521  Tue Apr 07 00:13:02 PDT 2009   \n",
      "26             26        0  1468206515  Tue Apr 07 00:17:42 PDT 2009   \n",
      "27             27        0  1468217987  Tue Apr 07 00:21:35 PDT 2009   \n",
      "28             28        0  1468233946  Tue Apr 07 00:27:08 PDT 2009   \n",
      "29             29        0  1468248952  Tue Apr 07 00:32:13 PDT 2009   \n",
      "...           ...      ...         ...                           ...   \n",
      "24971       24971        4  2193105382  Tue Jun 16 08:00:17 PDT 2009   \n",
      "24972       24972        4  2193121938  Tue Jun 16 08:01:38 PDT 2009   \n",
      "24973       24973        4  2193125073  Tue Jun 16 08:01:53 PDT 2009   \n",
      "24974       24974        4  2193155211  Tue Jun 16 08:04:18 PDT 2009   \n",
      "24975       24975        4  2193179229  Tue Jun 16 08:06:14 PDT 2009   \n",
      "24976       24976        4  2193182322  Tue Jun 16 08:06:29 PDT 2009   \n",
      "24977       24977        4  2193189268  Tue Jun 16 08:07:03 PDT 2009   \n",
      "24978       24978        4  2193221821  Tue Jun 16 08:09:45 PDT 2009   \n",
      "24979       24979        4  2193224855  Tue Jun 16 08:10:00 PDT 2009   \n",
      "24980       24980        4  2193254134  Tue Jun 16 08:12:22 PDT 2009   \n",
      "24981       24981        4  2193278105  Tue Jun 16 08:14:23 PDT 2009   \n",
      "24982       24982        4  2193282126  Tue Jun 16 08:14:43 PDT 2009   \n",
      "24983       24983        4  2193305502  Tue Jun 16 08:16:36 PDT 2009   \n",
      "24984       24984        4  2193319249  Tue Jun 16 08:17:44 PDT 2009   \n",
      "24985       24985        4  2193322665  Tue Jun 16 08:18:02 PDT 2009   \n",
      "24986       24986        4  2193345339  Tue Jun 16 08:19:56 PDT 2009   \n",
      "24987       24987        4  2193371250  Tue Jun 16 08:22:01 PDT 2009   \n",
      "24988       24988        4  2193374881  Tue Jun 16 08:22:19 PDT 2009   \n",
      "24989       24989        4  2193403814  Tue Jun 16 08:24:44 PDT 2009   \n",
      "24990       24990        4  2193427329  Tue Jun 16 08:26:39 PDT 2009   \n",
      "24991       24991        4  2193450427  Tue Jun 16 08:28:32 PDT 2009   \n",
      "24992       24992        4  2193453445  Tue Jun 16 08:28:47 PDT 2009   \n",
      "24993       24993        4  2193475408  Tue Jun 16 08:30:32 PDT 2009   \n",
      "24994       24994        4  2193478813  Tue Jun 16 08:30:48 PDT 2009   \n",
      "24995       24995        4  2193503480  Tue Jun 16 08:32:48 PDT 2009   \n",
      "24996       24996        4  2193526931  Tue Jun 16 08:34:42 PDT 2009   \n",
      "24997       24997        4  2193550768  Tue Jun 16 08:36:38 PDT 2009   \n",
      "24998       24998        4  2193553464  Tue Jun 16 08:36:51 PDT 2009   \n",
      "24999       24999        4  2193576573  Tue Jun 16 08:38:46 PDT 2009   \n",
      "25000       25000        4  2193602129  Tue Jun 16 08:40:50 PDT 2009   \n",
      "\n",
      "          QUERY             User  \\\n",
      "0      NO_QUERY  _TheSpecialOne_   \n",
      "1      NO_QUERY         ericg622   \n",
      "2      NO_QUERY           bgoers   \n",
      "3      NO_QUERY    msbutt3rfly14   \n",
      "4      NO_QUERY    SupernovaGirl   \n",
      "5      NO_QUERY           dreaaa   \n",
      "6      NO_QUERY           LAbite   \n",
      "7      NO_QUERY  kailashvasupati   \n",
      "8      NO_QUERY         javajive   \n",
      "9      NO_QUERY         MonikkaB   \n",
      "10     NO_QUERY    Smith_Cameron   \n",
      "11     NO_QUERY         BrianG2k   \n",
      "12     NO_QUERY      Hanster7705   \n",
      "13     NO_QUERY          mishkia   \n",
      "14     NO_QUERY  patrick_ritchie   \n",
      "15     NO_QUERY        dianapwns   \n",
      "16     NO_QUERY     Sallyroberts   \n",
      "17     NO_QUERY        eduardtee   \n",
      "18     NO_QUERY          gillana   \n",
      "19     NO_QUERY     LauraOrchard   \n",
      "20     NO_QUERY      tominlumban   \n",
      "21     NO_QUERY      paul_steele   \n",
      "22     NO_QUERY         KristySS   \n",
      "23     NO_QUERY     amiemccarron   \n",
      "24     NO_QUERY      emmaketurah   \n",
      "25     NO_QUERY   catherinestack   \n",
      "26     NO_QUERY       Jetske1977   \n",
      "27     NO_QUERY       salvadylan   \n",
      "28     NO_QUERY  charlierobinson   \n",
      "29     NO_QUERY    kathleenmary2   \n",
      "...         ...              ...   \n",
      "24971  NO_QUERY     birthgoddess   \n",
      "24972  NO_QUERY        TBJENKINS   \n",
      "24973  NO_QUERY            Zjack   \n",
      "24974  NO_QUERY        pixiejane   \n",
      "24975  NO_QUERY             jmt1   \n",
      "24976  NO_QUERY       karhenjhoy   \n",
      "24977  NO_QUERY        xcaromeLx   \n",
      "24978  NO_QUERY        kathyxxxx   \n",
      "24979  NO_QUERY       jetgirlart   \n",
      "24980  NO_QUERY      dennis_luis   \n",
      "24981  NO_QUERY       TheEdgeOkc   \n",
      "24982  NO_QUERY       leewilkins   \n",
      "24983  NO_QUERY     mccasey_1977   \n",
      "24984  NO_QUERY  lafayetteonline   \n",
      "24985  NO_QUERY   diegolikecrazy   \n",
      "24986  NO_QUERY     spacegrace19   \n",
      "24987  NO_QUERY   adambuckeridge   \n",
      "24988  NO_QUERY        MrsLoulou   \n",
      "24989  NO_QUERY       lyddiechoi   \n",
      "24990  NO_QUERY        whipzilla   \n",
      "24991  NO_QUERY      HelloGracey   \n",
      "24992  NO_QUERY   camilamariotto   \n",
      "24993  NO_QUERY       SamNiley11   \n",
      "24994  NO_QUERY         davoloid   \n",
      "24995  NO_QUERY         JConnell   \n",
      "24996  NO_QUERY   Killer_Burrito   \n",
      "24997  NO_QUERY        mlvlatina   \n",
      "24998  NO_QUERY     UrBaN_eLySsE   \n",
      "24999  NO_QUERY            Emm94   \n",
      "25000  NO_QUERY   RyanTrevMorris   \n",
      "\n",
      "                                                 Content  \n",
      "0      @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1      I had such a nice day. Too bad the rain comes ...  \n",
      "2                                           I'm so cold   \n",
      "3                            spencer is not a good guy.   \n",
      "4      @burgaw Ooooooh! *sealclap* See, I download sh...  \n",
      "5      throat is closing up and i had some string che...  \n",
      "6      Pepperoni rolls in L.A.?: I called Valentino's...  \n",
      "7      Dammit, episode 3 of Kings won't play for some...  \n",
      "8      @pratama Same iMac came out $320 more in Indon...  \n",
      "9      @paul_e_wog Wait...is it a game or just episod...  \n",
      "10     @hillary006 I'm sure everyone has ruined my gi...  \n",
      "11                         @lilbucknuts41 not an option   \n",
      "12                        i feel sick  too much icecream  \n",
      "13     aaaaand back to my literature review  At least...  \n",
      "14                          @featherinair call me back.   \n",
      "15              @alexbigman you left without saying hi!   \n",
      "16     @graemearcher I am sad about Innocent selling ...  \n",
      "17     *Sigh* Rain??? Why did you decide to show up? ...  \n",
      "18     Needs to stop sleeping all day.. Causes some p...  \n",
      "19                                     bored and lonely   \n",
      "20     Yo jimo i cant talk on aim anymore, its glitch...  \n",
      "21     Please watch this vid and tell me if you are n...  \n",
      "22     @RedGray Ah man- so sad, his cousin committed ...  \n",
      "23                                @Nightwyrm no not yet   \n",
      "24                               @GillianMe Yeah he was   \n",
      "25                         @laurenlenewx awww i'm sorry   \n",
      "26     @eri74 Goodmorning!  hahaha, let me guess .......  \n",
      "27     I'm going to put myself out of this misery and...  \n",
      "28     @Sir_Almo it wont let me play DOH keeps saying...  \n",
      "29     @iwouldificould How have you watched it? I tri...  \n",
      "...                                                  ...  \n",
      "24971  My 11YO son is off to see Africa: The Serenget...  \n",
      "24972             Just finished taking a nice long walk   \n",
      "24973                                   @MakingSense 18   \n",
      "24974  Loved the long wknd it was...5 days  bt it is ...  \n",
      "24975           hey it's Jeff  http://aweber.com/b/1huHV  \n",
      "24976  good night here in the Philippines and good mo...  \n",
      "24977  @karenblakcullen yea he did! he is pretty hot ...  \n",
      "24978  @gregjames as long as you didnt wake her up, y...  \n",
      "24979  @dougwaltman  there is a tiny creature on your...  \n",
      "24980  @mikasounds if you like to support talented yo...  \n",
      "24981  @shadfar that's my baby boy!  His name is Stua...  \n",
      "24982  @subiectiv  I never cursed you. Its just one o...  \n",
      "24983                   @carolynaaa28 Ahhhh! How sweet!   \n",
      "24984  @christyjones Gotta make your own chex mix... ...  \n",
      "24985  @MariaLKanellis true, true, also gets you into...  \n",
      "24986  helllllo twitter! #iremember playing #haveyoue...  \n",
      "24987  @spiralhosting Cool , I look forward to it  Lo...  \n",
      "24988              @phdinparenting It was a great post!   \n",
      "24989  is off to lesson then ortho appt.  Dreading to...  \n",
      "24990  - had a great time with some of the best peopl...  \n",
      "24991  Exploring the world of Twitter   Listening to ...  \n",
      "24992         @alicebarrooss happy birthday, alice!!!!!   \n",
      "24993                                  @francii_ me too   \n",
      "24994  @theskink And in car?  On street via Mobile?  ...  \n",
      "24995  @theokk don't know what you could possibly mea...  \n",
      "24996  Okay so the plan is to go to hot topic and buy...  \n",
      "24997                   @asimkovsky thanks for the info   \n",
      "24998  @FoSho174 LMAO @ capture the flag..bless their...  \n",
      "24999  Is back from getting the new jonas brothers cd...  \n",
      "25000  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[25001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custome dataset class for twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        sample = {'content': content, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)\n",
    "\n",
    "# Load word embeddings from pretrained embeddings file en-cw.txt, courtesy CS224N\n",
    "# For final, use Word2Vec embeddings, but for now this should suffice\n",
    "word_vectors = {}\n",
    "for line in open('./data/en-cw.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token IDs from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize datasets with token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize(train_x_raw)\n",
    "\n",
    "dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "test_x = vectorize(test_x_raw)\n",
    "\n",
    "train_dataset = TweetDataset(train_x, train_y)\n",
    "dev_dataset = TweetDataset(dev_x, dev_y)\n",
    "test_dataset = TweetDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), 50)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class definition, courtesy https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embeddings.shape[1])) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.sigmoid(self.fc(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, embeddings, hidden_dim, n_layers, output_dim, bidirectional = True, dropout = 0.5):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "#         self.rnn = nn.LSTM(embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         #x = [sent len, batch size]\n",
    "        \n",
    "#         embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "#         #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "#         output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "#         #output = [sent len, batch size, hid dim * num directions]\n",
    "#         #hidden = [num layers * num directions, batch size, hid dim]\n",
    "#         #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "#         #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "#         #and apply dropout\n",
    "        \n",
    "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "#         #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "#         return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_dim = 256\n",
    "# output_dim = 1\n",
    "# n_layers = 2\n",
    "\n",
    "# RNN_model = RNN(embeddings_matrix, hidden_dim, n_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 100\n",
    "filter_sizes = [1, 2, 3, 4]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "CNN_model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and criterion (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy score, i.e. percentage correct per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        #print (\"Training on batch #\" + str(batchnum))\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        if train_x.shape[1] == 1: continue\n",
    "        #print (train_y.view(-1).shape)\n",
    "        \n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        #print (predictions.shape)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "# train(RNN_model, enumerate(train_loader), optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            #print (predictions)\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_loader), epoch_acc / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, dev_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.711 | Train Acc: 50.67% | Val. Loss: 0.693 | Val. Acc: 50.00% |\n",
      "| Epoch: 02 | Train Loss: 0.695 | Train Acc: 49.92% | Val. Loss: 0.693 | Val. Acc: 50.00% |\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(CNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(CNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "test_loss, test_acc = evaluate(CNN_model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN Model over 5 epochs (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# optimizer = optim.Adam(RNN_model.parameters())\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RNN_model = RNN_model.to(device)\n",
    "\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "# N_EPOCHS = 5 # Since the RNN is so slow\n",
    "\n",
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "    \n",
    "# dev_loader = DataLoader(dev_dataset,\n",
    "#                   batch_size=40,\n",
    "#                   shuffle=False,\n",
    "#                   num_workers=4\n",
    "#                  # pin_memory=True # CUDA only\n",
    "#                  )\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "\n",
    "#     train_loss, train_acc = train(RNN_model, train_loader, optimizer, criterion)\n",
    "#     valid_loss, valid_acc = evaluate(RNN_model, dev_loader, criterion)\n",
    "    \n",
    "#     print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
