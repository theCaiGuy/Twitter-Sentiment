{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  Pos_Neg          ID                          Date  \\\n",
      "0                0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
      "1                1        0  1467812964  Mon Apr 06 22:20:22 PDT 2009   \n",
      "2                2        0  1467815988  Mon Apr 06 22:21:09 PDT 2009   \n",
      "3                3        0  1467820206  Mon Apr 06 22:22:13 PDT 2009   \n",
      "4                4        0  1467823851  Mon Apr 06 22:23:09 PDT 2009   \n",
      "5                5        0  1467834053  Mon Apr 06 22:25:52 PDT 2009   \n",
      "6                6        0  1467836500  Mon Apr 06 22:26:28 PDT 2009   \n",
      "7                7        0  1467838419  Mon Apr 06 22:27:00 PDT 2009   \n",
      "8                8        0  1467841832  Mon Apr 06 22:27:55 PDT 2009   \n",
      "9                9        0  1467843624  Mon Apr 06 22:28:24 PDT 2009   \n",
      "10              10        0  1467853356  Mon Apr 06 22:30:54 PDT 2009   \n",
      "11              11        0  1467856919  Mon Apr 06 22:31:50 PDT 2009   \n",
      "12              12        0  1467859820  Mon Apr 06 22:32:36 PDT 2009   \n",
      "13              13        0  1467863072  Mon Apr 06 22:33:25 PDT 2009   \n",
      "14              14        0  1467871661  Mon Apr 06 22:35:41 PDT 2009   \n",
      "15              15        0  1467873004  Mon Apr 06 22:36:03 PDT 2009   \n",
      "16              16        0  1467876652  Mon Apr 06 22:37:03 PDT 2009   \n",
      "17              17        0  1467879984  Mon Apr 06 22:37:57 PDT 2009   \n",
      "18              18        0  1467882491  Mon Apr 06 22:38:37 PDT 2009   \n",
      "19              19        0  1467891826  Mon Apr 06 22:41:07 PDT 2009   \n",
      "20              20        0  1467894600  Mon Apr 06 22:41:51 PDT 2009   \n",
      "21              21        0  1467896777  Mon Apr 06 22:42:28 PDT 2009   \n",
      "22              22        0  1467899753  Mon Apr 06 22:43:18 PDT 2009   \n",
      "23              23        0  1467905378  Mon Apr 06 22:44:52 PDT 2009   \n",
      "24              24        0  1467909222  Mon Apr 06 22:45:53 PDT 2009   \n",
      "25              25        0  1467913111  Mon Apr 06 22:46:57 PDT 2009   \n",
      "26              26        0  1467917177  Mon Apr 06 22:48:08 PDT 2009   \n",
      "27              27        0  1467919762  Mon Apr 06 22:48:53 PDT 2009   \n",
      "28              28        0  1467926632  Mon Apr 06 22:50:51 PDT 2009   \n",
      "29              29        0  1467929915  Mon Apr 06 22:51:46 PDT 2009   \n",
      "...            ...      ...         ...                           ...   \n",
      "99971        99971        4  2193474627  Tue Jun 16 08:30:28 PDT 2009   \n",
      "99972        99972        4  2193475408  Tue Jun 16 08:30:32 PDT 2009   \n",
      "99973        99973        4  2193476073  Tue Jun 16 08:30:35 PDT 2009   \n",
      "99974        99974        4  2193476829  Tue Jun 16 08:30:39 PDT 2009   \n",
      "99975        99975        4  2193477741  Tue Jun 16 08:30:43 PDT 2009   \n",
      "99976        99976        4  2193478813  Tue Jun 16 08:30:48 PDT 2009   \n",
      "99977        99977        4  2193501460  Tue Jun 16 08:32:38 PDT 2009   \n",
      "99978        99978        4  2193502163  Tue Jun 16 08:32:41 PDT 2009   \n",
      "99979        99979        4  2193502915  Tue Jun 16 08:32:45 PDT 2009   \n",
      "99980        99980        4  2193503480  Tue Jun 16 08:32:48 PDT 2009   \n",
      "99981        99981        4  2193503964  Tue Jun 16 08:32:50 PDT 2009   \n",
      "99982        99982        4  2193525430  Tue Jun 16 08:34:35 PDT 2009   \n",
      "99983        99983        4  2193526158  Tue Jun 16 08:34:38 PDT 2009   \n",
      "99984        99984        4  2193526931  Tue Jun 16 08:34:42 PDT 2009   \n",
      "99985        99985        4  2193527854  Tue Jun 16 08:34:46 PDT 2009   \n",
      "99986        99986        4  2193528475  Tue Jun 16 08:34:50 PDT 2009   \n",
      "99987        99987        4  2193529266  Tue Jun 16 08:34:54 PDT 2009   \n",
      "99988        99988        4  2193550768  Tue Jun 16 08:36:38 PDT 2009   \n",
      "99989        99989        4  2193551468  Tue Jun 16 08:36:42 PDT 2009   \n",
      "99990        99990        4  2193552024  Tue Jun 16 08:36:44 PDT 2009   \n",
      "99991        99991        4  2193552668  Tue Jun 16 08:36:48 PDT 2009   \n",
      "99992        99992        4  2193553464  Tue Jun 16 08:36:51 PDT 2009   \n",
      "99993        99993        4  2193554237  Tue Jun 16 08:36:55 PDT 2009   \n",
      "99994        99994        4  2193575004  Tue Jun 16 08:38:38 PDT 2009   \n",
      "99995        99995        4  2193575839  Tue Jun 16 08:38:42 PDT 2009   \n",
      "99996        99996        4  2193576573  Tue Jun 16 08:38:46 PDT 2009   \n",
      "99997        99997        4  2193577586  Tue Jun 16 08:38:51 PDT 2009   \n",
      "99998        99998        4  2193578151  Tue Jun 16 08:38:54 PDT 2009   \n",
      "99999        99999        4  2193578847  Tue Jun 16 08:38:57 PDT 2009   \n",
      "100000      100000        4  2193602129  Tue Jun 16 08:40:50 PDT 2009   \n",
      "\n",
      "           QUERY             User  \\\n",
      "0       NO_QUERY  _TheSpecialOne_   \n",
      "1       NO_QUERY   lovesongwriter   \n",
      "2       NO_QUERY         merisssa   \n",
      "3       NO_QUERY         peacoats   \n",
      "4       NO_QUERY         ericg622   \n",
      "5       NO_QUERY       thelazyboy   \n",
      "6       NO_QUERY   natalieantipas   \n",
      "7       NO_QUERY        polarna10   \n",
      "8       NO_QUERY           bgoers   \n",
      "9       NO_QUERY      kellireneez   \n",
      "10      NO_QUERY         dbmendel   \n",
      "11      NO_QUERY         jhenkens   \n",
      "12      NO_QUERY    msbutt3rfly14   \n",
      "13      NO_QUERY         Artiel87   \n",
      "14      NO_QUERY         ciairuhh   \n",
      "15      NO_QUERY         omgitsjo   \n",
      "16      NO_QUERY    SupernovaGirl   \n",
      "17      NO_QUERY      marybacchus   \n",
      "18      NO_QUERY   Stereo_Skyline   \n",
      "19      NO_QUERY      elamparuthi   \n",
      "20      NO_QUERY           dreaaa   \n",
      "21      NO_QUERY    EricaLeigh777   \n",
      "22      NO_QUERY       Sheezy3380   \n",
      "23      NO_QUERY     raymondroman   \n",
      "24      NO_QUERY           LAbite   \n",
      "25      NO_QUERY           nssmom   \n",
      "26      NO_QUERY         nchokkan   \n",
      "27      NO_QUERY     Properteacup   \n",
      "28      NO_QUERY  kailashvasupati   \n",
      "29      NO_QUERY       zeegirl602   \n",
      "...          ...              ...   \n",
      "99971   NO_QUERY         HollyEgg   \n",
      "99972   NO_QUERY       SamNiley11   \n",
      "99973   NO_QUERY       xLukesterx   \n",
      "99974   NO_QUERY     Septimus1812   \n",
      "99975   NO_QUERY      just_chalie   \n",
      "99976   NO_QUERY         davoloid   \n",
      "99977   NO_QUERY    chloethelwell   \n",
      "99978   NO_QUERY       JohnWeston   \n",
      "99979   NO_QUERY      asifrehmani   \n",
      "99980   NO_QUERY         JConnell   \n",
      "99981   NO_QUERY     babydollniki   \n",
      "99982   NO_QUERY  MandeeLeigh1911   \n",
      "99983   NO_QUERY   virtuosoencore   \n",
      "99984   NO_QUERY   Killer_Burrito   \n",
      "99985   NO_QUERY         1LisAnne   \n",
      "99986   NO_QUERY            Shash   \n",
      "99987   NO_QUERY      rohinkallat   \n",
      "99988   NO_QUERY        mlvlatina   \n",
      "99989   NO_QUERY      slimgoodies   \n",
      "99990   NO_QUERY          bdottie   \n",
      "99991   NO_QUERY     CintiaXimena   \n",
      "99992   NO_QUERY     UrBaN_eLySsE   \n",
      "99993   NO_QUERY     krazikiddx09   \n",
      "99994   NO_QUERY  say_my_name_too   \n",
      "99995   NO_QUERY        bendotorg   \n",
      "99996   NO_QUERY            Emm94   \n",
      "99997   NO_QUERY  IsabellesTravel   \n",
      "99998   NO_QUERY  victoriaquinnxo   \n",
      "99999   NO_QUERY       RobFoxKerr   \n",
      "100000  NO_QUERY   RyanTrevMorris   \n",
      "\n",
      "                                                  Content  \n",
      "0       @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1       Hollis' death scene will hurt me severely to w...  \n",
      "2       thought sleeping in was an option tomorrow but...  \n",
      "3       is strangely sad about LiLo and SamRo breaking...  \n",
      "4       I had such a nice day. Too bad the rain comes ...  \n",
      "5       sleep soon... i just hate saying bye and see y...  \n",
      "6       so rylee,grace...wana go steve's party or not?...  \n",
      "7       @jacobsummers Sorry  tell them mea culpa from ...  \n",
      "8                                            I'm so cold   \n",
      "9        laid around too much today... now my head hurts   \n",
      "10      Picked Mich St to win it all from the get go. ...  \n",
      "11      @ColinDeMar Far too out of the way for rail.  ...  \n",
      "12                            spencer is not a good guy.   \n",
      "13                                             @mandayyy   \n",
      "14      I miss you twitter. My phone broke, now I'm us...  \n",
      "15      I miss him.  Can't wait to celebrate the Tar H...  \n",
      "16      @burgaw Ooooooh! *sealclap* See, I download sh...  \n",
      "17      @mathewsmichael  i agree... the jobros dont up...  \n",
      "18      @ThaStevieG but what I really want is my old b...  \n",
      "19      amazon s3 plugin not worked in my website . it...  \n",
      "20      throat is closing up and i had some string che...  \n",
      "21      @MizzChievouz Hey girl. The site is back.  Gir...  \n",
      "22                       New Testament Test at 9:30 am     \n",
      "23      Wow. The most depressing thing in the world is...  \n",
      "24      Pepperoni rolls in L.A.?: I called Valentino's...  \n",
      "25      #3 woke up and was having an accident - &quot;...  \n",
      "26      @KishoreK this is strange, illegal torrents av...  \n",
      "27      @sleep_til_noon I did, it was the only one lef...  \n",
      "28      Dammit, episode 3 of Kings won't play for some...  \n",
      "29      My bathtub drain is fired: it haz 1 job 2 do, ...  \n",
      "...                                                   ...  \n",
      "99971   @Suzika sheesh! that's a lot of money! You've ...  \n",
      "99972                                   @francii_ me too   \n",
      "99973   @NicciLuvsMollie no problem ;) and awww  I'm s...  \n",
      "99974   @radagast22 It's not the 'nillas I worry about...  \n",
      "99975   @Elle_333 @jenfafer midnight showing peoples, ...  \n",
      "99976   @theskink And in car?  On street via Mobile?  ...  \n",
      "99977                                    103 Followers!!   \n",
      "99978           http://twitpic.com/7jomc - Party invites   \n",
      "99979   @hwaterman Thanks for the all the advice Heath...  \n",
      "99980   @theokk don't know what you could possibly mea...  \n",
      "99981   @ellerysweet Awwh go take a nap my love!! If I...  \n",
      "99982   Watching pride and prejudice again &lt;3 then ...  \n",
      "99983                    @miss_magenta  Congratulations.   \n",
      "99984   Okay so the plan is to go to hot topic and buy...  \n",
      "99985   @AmyLynne18 I love you.. can't wait for this w...  \n",
      "99986                         @linksforluv  you betcha!!   \n",
      "99987                   @FrancyneWalker You go Francyne!   \n",
      "99988                    @asimkovsky thanks for the info   \n",
      "99989   @hillsmarket Sweet.  Thanks for posting that. ...  \n",
      "99990           What a pretty day  &quot;Just smile&quot;  \n",
      "99991   taking a twit break...a lil blown about my bos...  \n",
      "99992   @FoSho174 LMAO @ capture the flag..bless their...  \n",
      "99993   Getting ready to have a picnic in the park w/ ...  \n",
      "99994                  @say_my_name TRAITOR!!!! love you   \n",
      "99995   Largest/fastest hotspot in U.S.A. is located i...  \n",
      "99996   Is back from getting the new jonas brothers cd...  \n",
      "99997   #Traveltuesday @GuyNGirlTravels Because their ...  \n",
      "99998    done la examen! easy peasy  so proud of myself!!  \n",
      "99999   You heard it here first -- We're having a girl...  \n",
      "100000  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[100001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custome dataset class for twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        sample = {'content': content, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)\n",
    "\n",
    "# Load word embeddings from pretrained embeddings file en-cw.txt, courtesy CS224N\n",
    "# For final, use Word2Vec embeddings, but for now this should suffice\n",
    "word_vectors = {}\n",
    "for line in open('./data/en-cw.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token IDs from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize datasets with token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize(train_x_raw)\n",
    "\n",
    "dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "test_x = vectorize(test_x_raw)\n",
    "\n",
    "train_dataset = TweetDataset(train_x, train_y)\n",
    "dev_dataset = TweetDataset(dev_x, dev_y)\n",
    "test_dataset = TweetDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), 50)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class definition, courtesy https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embeddings.shape[1])) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim, n_layers, output_dim, bidirectional = True, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.rnn = nn.LSTM(embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "\n",
    "RNN_model = RNN(embeddings_matrix, hidden_dim, n_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 100\n",
    "filter_sizes = [1, 2, 3, 4]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "CNN_model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and criterion (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy score, i.e. percentage correct per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        #print (\"Training on batch #\" + str(batchnum))\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        if train_x.shape[1] == 1: continue\n",
    "        #print (train_y.view(-1).shape)\n",
    "        \n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        #print (predictions.shape)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "# train(RNN_model, enumerate(train_loader), optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            #print (predictions)\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_loader), epoch_acc / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, dev_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=40,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  batch_size=40,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(CNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(CNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN Model over 5 epochs (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(RNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "RNN_model = RNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 5 # Since the RNN is so slow\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=40,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  batch_size=40,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(RNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(RNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
