{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from TweetDataset import TweetDataset\n",
    "from vocab import VocabEntry\n",
    "from convblock import ConvBlock\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "EMBED_LEN = 100\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'\n",
    "\n",
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "#print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)\n",
    "\n",
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Load character embeddings from pretrained embeddings file char-embeddings.txt, courtesy github user minimaxir\n",
    "# char_vectors = {}\n",
    "# i = 0\n",
    "# tokens = open('./data/metadata.txt').readlines()\n",
    "# embeddings = open('./data/character-embeddings.txt').readlines()\n",
    "\n",
    "# for i in range(len(tokens)):\n",
    "#     token = tokens[i].strip()\n",
    "#     sp = embeddings[i].strip().split()\n",
    "#     char_vectors[token] = [float(x) for x in sp]\n",
    "\n",
    "# print (char_vectors['three'])\n",
    "# print (len(char_vectors))\n",
    "# print (len(char_vectors['three']))\n",
    "\n",
    "char_vectors = {}\n",
    "for line in open('./glove.6B/glove.6B.100d.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    if len(sp) == 0: continue\n",
    "    char_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    \n",
    "print (char_vectors)\n",
    "print (len(char_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize(train_x_raw)\n",
    "\n",
    "dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "test_x = vectorize(test_x_raw)\n",
    "\n",
    "train_dataset = TweetDataset(train_x, train_y)\n",
    "dev_dataset = TweetDataset(dev_x, dev_y)\n",
    "test_dataset = TweetDataset(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), EMBED_LEN)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in char_vectors:\n",
    "        embeddings_matrix[i] = char_vectors[token]\n",
    "    elif token.lower() in char_vectors:\n",
    "        embeddings_matrix[i] = char_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "#         self.convs = nn.ModuleList([nn.Conv1d(in_channels=EMBED_LEN, out_channels=n_filters, kernel_size=fs) for fs in filter_sizes])\n",
    "        \n",
    "#         for conv in self.convs:\n",
    "#             torch.nn.init.xavier_uniform_(conv.weight)\n",
    "        \n",
    "#         self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "#         torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         #x = [sent len, batch size]\n",
    "        \n",
    "#         x = x.permute(1, 0)\n",
    "                \n",
    "#         #x = [batch size, sent len]\n",
    "        \n",
    "#         embedded = self.embedding(x).permute(0, 2, 1)\n",
    "                \n",
    "#         #embedded = [batch size, emb dim, sent len]\n",
    "#         print (embedded.shape)\n",
    "# #         embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "#         #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "#         conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "#         #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "#         pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "#         #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "#         cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "#         #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "#         return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, embeddings, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = embeddings.shape[1]\n",
    "        \n",
    "        # Freeze pretrained GloVe embeddings \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=embeddings.shape[1], out_channels=256, kernel_size=5, padding = 2)\n",
    "        #torch.nn.init.xavier_uniform_(self.conv.weight)\n",
    "        self.block = ConvBlock(256, 256)\n",
    "        self.block2 = ConvBlock(256, 256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(5 * 256, 1)\n",
    "        #torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x, print_sizes = False):\n",
    "        # x.shape = (sent_len, batch_size)\n",
    "        x = x.permute(1, 0)\n",
    "        if print_sizes: print (\"x.shape: \" + str(x.shape))\n",
    "        # x.shape = (batch_size, sent_len)\n",
    "        \n",
    "        x_emb = self.embedding(x).permute(0, 2, 1)\n",
    "        if print_sizes: print (\"x_emb.shape: \" + str(x_emb.shape))\n",
    "        # x_emb.shape = (batch_size, emb_size, sent_len)\n",
    "        \n",
    "        x_conv = self.conv(x_emb)\n",
    "        # x_conv.shape = (batch_size, 128, sent_len)\n",
    "        if print_sizes: print (\"x_conv.shape: \" + str(x_conv.shape))\n",
    "        \n",
    "        x_block = self.block.forward(x_conv)\n",
    "        # x_block.shape = (batch_size, 128, sent_len / 2)\n",
    "        if print_sizes: print (\"x_block.shape: \" + str(x_block.shape))\n",
    "            \n",
    "        x_block_2 = self.block2.forward(x_block)\n",
    "        # x_block.shape = (batch_size, 256, sent_len / 4)\n",
    "        if print_sizes: print (\"x_block_2.shape: \" + str(x_block_2.shape))\n",
    "\n",
    "        x_cat = x_block_2.view(-1, x_block_2.shape[1] * x_block_2.shape[2])\n",
    "        # x_cat.shape = (batch_size, 128 * sent_len / 2)\n",
    "        if print_sizes: print (\"x_cat.shape: \" + str(x_cat.shape))\n",
    "            \n",
    "        x_fc = self.fc(self.dropout(x_cat))\n",
    "        # x_fc.shape = (batch_size, 1)\n",
    "        if print_sizes: print (\"x_fc.shape: \" + str(x_fc.shape))\n",
    "\n",
    "        return x_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepCNN(embeddings_matrix)\n",
    "# x = torch.tensor(np.zeros((20, 128)), dtype = torch.long)\n",
    "# model.forward(x, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_filters = 100\n",
    "# filter_sizes = [1, 2, 3, 4]\n",
    "# output_dim = 1\n",
    "# dropout = 0.5\n",
    "\n",
    "# CNN_model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)\n",
    "CNN_model = DeepCNN(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        #print (\"Training on batch #\" + str(batchnum))\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x.shape)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        if train_x.shape[1] == 1: continue\n",
    "        #print (train_y.view(-1).shape)\n",
    "\n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        #print (predictions.shape)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)\n",
    "\n",
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            #print (torch.round(predictions))\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_loader), epoch_acc / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.843 | Train Acc: 51.25% | Val. Loss: 0.702 | Val. Acc: 53.49% |\n",
      "| Epoch: 02 | Train Loss: 0.820 | Train Acc: 54.86% | Val. Loss: 1.708 | Val. Acc: 51.65% |\n",
      "| Epoch: 03 | Train Loss: 0.851 | Train Acc: 53.90% | Val. Loss: 0.977 | Val. Acc: 50.28% |\n",
      "| Epoch: 04 | Train Loss: 0.781 | Train Acc: 55.23% | Val. Loss: 3.271 | Val. Acc: 50.09% |\n",
      "| Epoch: 05 | Train Loss: 0.806 | Train Acc: 53.83% | Val. Loss: 1.747 | Val. Acc: 51.07% |\n",
      "| Epoch: 06 | Train Loss: 0.782 | Train Acc: 58.09% | Val. Loss: 1.052 | Val. Acc: 44.05% |\n",
      "| Epoch: 07 | Train Loss: 0.814 | Train Acc: 53.12% | Val. Loss: 0.851 | Val. Acc: 48.83% |\n",
      "| Epoch: 08 | Train Loss: 0.812 | Train Acc: 53.58% | Val. Loss: 0.674 | Val. Acc: 59.98% |\n",
      "| Epoch: 09 | Train Loss: 0.729 | Train Acc: 59.85% | Val. Loss: 0.797 | Val. Acc: 53.57% |\n",
      "| Epoch: 10 | Train Loss: 0.738 | Train Acc: 59.63% | Val. Loss: 0.911 | Val. Acc: 54.50% |\n",
      "| Epoch: 11 | Train Loss: 0.756 | Train Acc: 57.98% | Val. Loss: 0.934 | Val. Acc: 51.74% |\n",
      "| Epoch: 12 | Train Loss: 0.738 | Train Acc: 59.78% | Val. Loss: 0.750 | Val. Acc: 53.28% |\n",
      "| Epoch: 13 | Train Loss: 0.725 | Train Acc: 60.50% | Val. Loss: 0.666 | Val. Acc: 66.99% |\n",
      "| Epoch: 14 | Train Loss: 0.744 | Train Acc: 54.88% | Val. Loss: 0.743 | Val. Acc: 58.60% |\n",
      "| Epoch: 15 | Train Loss: 0.724 | Train Acc: 59.58% | Val. Loss: 0.800 | Val. Acc: 62.98% |\n",
      "| Epoch: 16 | Train Loss: 0.704 | Train Acc: 60.00% | Val. Loss: 0.832 | Val. Acc: 56.72% |\n",
      "| Epoch: 17 | Train Loss: 0.739 | Train Acc: 61.04% | Val. Loss: 0.864 | Val. Acc: 53.91% |\n",
      "| Epoch: 18 | Train Loss: 0.701 | Train Acc: 60.51% | Val. Loss: 1.015 | Val. Acc: 53.66% |\n",
      "| Epoch: 19 | Train Loss: 0.732 | Train Acc: 58.85% | Val. Loss: 0.624 | Val. Acc: 67.52% |\n",
      "| Epoch: 20 | Train Loss: 0.703 | Train Acc: 62.10% | Val. Loss: 0.629 | Val. Acc: 66.35% |\n",
      "| Epoch: 21 | Train Loss: 0.675 | Train Acc: 59.71% | Val. Loss: 0.668 | Val. Acc: 66.68% |\n",
      "| Epoch: 22 | Train Loss: 0.720 | Train Acc: 63.29% | Val. Loss: 1.050 | Val. Acc: 53.79% |\n",
      "| Epoch: 23 | Train Loss: 0.644 | Train Acc: 62.67% | Val. Loss: 0.758 | Val. Acc: 64.74% |\n",
      "| Epoch: 24 | Train Loss: 0.731 | Train Acc: 63.04% | Val. Loss: 0.666 | Val. Acc: 56.85% |\n",
      "| Epoch: 25 | Train Loss: 0.688 | Train Acc: 61.97% | Val. Loss: 0.733 | Val. Acc: 67.50% |\n",
      "| Epoch: 26 | Train Loss: 0.646 | Train Acc: 62.86% | Val. Loss: 0.710 | Val. Acc: 58.84% |\n",
      "| Epoch: 27 | Train Loss: 0.690 | Train Acc: 64.97% | Val. Loss: 2571.921 | Val. Acc: 64.08% |\n",
      "| Epoch: 28 | Train Loss: 0.640 | Train Acc: 63.64% | Val. Loss: 7036.100 | Val. Acc: 67.16% |\n",
      "| Epoch: 29 | Train Loss: 0.679 | Train Acc: 65.91% | Val. Loss: 1.075 | Val. Acc: 53.40% |\n",
      "| Epoch: 30 | Train Loss: 0.639 | Train Acc: 63.25% | Val. Loss: 0.795 | Val. Acc: 67.81% |\n",
      "| Epoch: 31 | Train Loss: 0.663 | Train Acc: 66.13% | Val. Loss: 21.983 | Val. Acc: 59.95% |\n",
      "| Epoch: 32 | Train Loss: 0.655 | Train Acc: 63.29% | Val. Loss: 1.215 | Val. Acc: 51.80% |\n",
      "| Epoch: 33 | Train Loss: 0.642 | Train Acc: 66.68% | Val. Loss: 0.702 | Val. Acc: 66.15% |\n",
      "| Epoch: 34 | Train Loss: 0.657 | Train Acc: 63.20% | Val. Loss: 0.663 | Val. Acc: 64.43% |\n",
      "| Epoch: 35 | Train Loss: 0.629 | Train Acc: 66.17% | Val. Loss: 0.689 | Val. Acc: 65.62% |\n",
      "| Epoch: 36 | Train Loss: 0.638 | Train Acc: 64.44% | Val. Loss: 1.433 | Val. Acc: 64.09% |\n",
      "| Epoch: 37 | Train Loss: 0.649 | Train Acc: 67.17% | Val. Loss: 0.865 | Val. Acc: 56.70% |\n",
      "| Epoch: 38 | Train Loss: 0.667 | Train Acc: 63.45% | Val. Loss: 3.955 | Val. Acc: 60.97% |\n",
      "| Epoch: 39 | Train Loss: 0.627 | Train Acc: 66.70% | Val. Loss: 0.667 | Val. Acc: 66.65% |\n",
      "| Epoch: 40 | Train Loss: 0.673 | Train Acc: 63.65% | Val. Loss: 12.077 | Val. Acc: 51.68% |\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 40\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  batch_size=128,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(CNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(CNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.440 | Test Acc: 50.57% |\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "test_loss, test_acc = evaluate(CNN_model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
