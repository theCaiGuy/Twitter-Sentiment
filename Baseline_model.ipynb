{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  Pos_Neg          ID                          Date  \\\n",
      "0               0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
      "1               1        0  1467853356  Mon Apr 06 22:30:54 PDT 2009   \n",
      "2               2        0  1467894600  Mon Apr 06 22:41:51 PDT 2009   \n",
      "3               3        0  1467932208  Mon Apr 06 22:52:25 PDT 2009   \n",
      "4               4        0  1467972262  Mon Apr 06 23:03:39 PDT 2009   \n",
      "5               5        0  1468007877  Mon Apr 06 23:13:57 PDT 2009   \n",
      "6               6        0  1468047066  Mon Apr 06 23:26:06 PDT 2009   \n",
      "7               7        0  1468085048  Mon Apr 06 23:37:52 PDT 2009   \n",
      "8               8        0  1468121466  Mon Apr 06 23:49:56 PDT 2009   \n",
      "9               9        0  1468155926  Tue Apr 07 00:01:16 PDT 2009   \n",
      "10             10        0  1468192521  Tue Apr 07 00:13:02 PDT 2009   \n",
      "11             11        0  1468226377  Tue Apr 07 00:24:32 PDT 2009   \n",
      "12             12        0  1468262100  Tue Apr 07 00:36:46 PDT 2009   \n",
      "13             13        0  1468298918  Tue Apr 07 00:49:44 PDT 2009   \n",
      "14             14        0  1468336500  Tue Apr 07 01:03:03 PDT 2009   \n",
      "15             15        0  1468371395  Tue Apr 07 01:15:42 PDT 2009   \n",
      "16             16        0  1468401268  Tue Apr 07 01:26:36 PDT 2009   \n",
      "17             17        0  1468440198  Tue Apr 07 01:40:56 PDT 2009   \n",
      "18             18        0  1468470368  Tue Apr 07 01:52:04 PDT 2009   \n",
      "19             19        0  1468504014  Tue Apr 07 02:04:24 PDT 2009   \n",
      "20             20        0  1468543053  Tue Apr 07 02:18:49 PDT 2009   \n",
      "21             21        0  1468575832  Tue Apr 07 02:30:32 PDT 2009   \n",
      "22             22        0  1468617899  Tue Apr 07 02:45:41 PDT 2009   \n",
      "23             23        0  1468649524  Tue Apr 07 02:57:11 PDT 2009   \n",
      "24             24        0  1468686594  Tue Apr 07 03:09:58 PDT 2009   \n",
      "25             25        0  1468721137  Tue Apr 07 03:21:55 PDT 2009   \n",
      "26             26        0  1468755779  Tue Apr 07 03:33:42 PDT 2009   \n",
      "27             27        0  1468791642  Tue Apr 07 03:45:25 PDT 2009   \n",
      "28             28        0  1468833927  Tue Apr 07 03:58:41 PDT 2009   \n",
      "29             29        0  1468873358  Tue Apr 07 04:10:37 PDT 2009   \n",
      "...           ...      ...         ...                           ...   \n",
      "9971         9971        4  2192357003  Tue Jun 16 06:54:56 PDT 2009   \n",
      "9972         9972        4  2192397932  Tue Jun 16 06:58:47 PDT 2009   \n",
      "9973         9973        4  2192425187  Tue Jun 16 07:01:09 PDT 2009   \n",
      "9974         9974        4  2192514568  Tue Jun 16 07:09:03 PDT 2009   \n",
      "9975         9975        4  2192559582  Tue Jun 16 07:12:59 PDT 2009   \n",
      "9976         9976        4  2192584392  Tue Jun 16 07:15:13 PDT 2009   \n",
      "9977         9977        4  2192628539  Tue Jun 16 07:19:08 PDT 2009   \n",
      "9978         9978        4  2192673182  Tue Jun 16 07:23:05 PDT 2009   \n",
      "9979         9979        4  2192722706  Tue Jun 16 07:27:24 PDT 2009   \n",
      "9980         9980        4  2192768974  Tue Jun 16 07:31:22 PDT 2009   \n",
      "9981         9981        4  2192811085  Tue Jun 16 07:35:01 PDT 2009   \n",
      "9982         9982        4  2192839605  Tue Jun 16 07:37:33 PDT 2009   \n",
      "9983         9983        4  2192886098  Tue Jun 16 07:41:36 PDT 2009   \n",
      "9984         9984        4  2192935901  Tue Jun 16 07:45:55 PDT 2009   \n",
      "9985         9985        4  2192961558  Tue Jun 16 07:48:07 PDT 2009   \n",
      "9986         9986        4  2193007920  Tue Jun 16 07:52:05 PDT 2009   \n",
      "9987         9987        4  2193053613  Tue Jun 16 07:55:59 PDT 2009   \n",
      "9988         9988        4  2193082128  Tue Jun 16 07:58:23 PDT 2009   \n",
      "9989         9989        4  2193123166  Tue Jun 16 08:01:44 PDT 2009   \n",
      "9990         9990        4  2193179229  Tue Jun 16 08:06:14 PDT 2009   \n",
      "9991         9991        4  2193190935  Tue Jun 16 08:07:12 PDT 2009   \n",
      "9992         9992        4  2193254134  Tue Jun 16 08:12:22 PDT 2009   \n",
      "9993         9993        4  2193304392  Tue Jun 16 08:16:31 PDT 2009   \n",
      "9994         9994        4  2193322665  Tue Jun 16 08:18:02 PDT 2009   \n",
      "9995         9995        4  2193373009  Tue Jun 16 08:22:10 PDT 2009   \n",
      "9996         9996        4  2193427329  Tue Jun 16 08:26:39 PDT 2009   \n",
      "9997         9997        4  2193454592  Tue Jun 16 08:28:53 PDT 2009   \n",
      "9998         9998        4  2193503480  Tue Jun 16 08:32:48 PDT 2009   \n",
      "9999         9999        4  2193552024  Tue Jun 16 08:36:44 PDT 2009   \n",
      "10000       10000        4  2193602129  Tue Jun 16 08:40:50 PDT 2009   \n",
      "\n",
      "          QUERY             User  \\\n",
      "0      NO_QUERY  _TheSpecialOne_   \n",
      "1      NO_QUERY         dbmendel   \n",
      "2      NO_QUERY           dreaaa   \n",
      "3      NO_QUERY        rachelgab   \n",
      "4      NO_QUERY    Smith_Cameron   \n",
      "5      NO_QUERY           John2x   \n",
      "6      NO_QUERY        dianapwns   \n",
      "7      NO_QUERY         policano   \n",
      "8      NO_QUERY      tominlumban   \n",
      "9      NO_QUERY          Jordyss   \n",
      "10     NO_QUERY   catherinestack   \n",
      "11     NO_QUERY        KristenCB   \n",
      "12     NO_QUERY          Fatty_D   \n",
      "13     NO_QUERY         xdokkenx   \n",
      "14     NO_QUERY       bethasaurr   \n",
      "15     NO_QUERY       tlelover91   \n",
      "16     NO_QUERY  soapdishsailing   \n",
      "17     NO_QUERY  courtniecupcake   \n",
      "18     NO_QUERY          jj_tins   \n",
      "19     NO_QUERY    sparksthetoby   \n",
      "20     NO_QUERY         alunjohn   \n",
      "21     NO_QUERY   RedVelvetHeart   \n",
      "22     NO_QUERY      jaredgunter   \n",
      "23     NO_QUERY     lewisholland   \n",
      "24     NO_QUERY    DivasMistress   \n",
      "25     NO_QUERY     Pitstopbunny   \n",
      "26     NO_QUERY        rpecknold   \n",
      "27     NO_QUERY         FenOswin   \n",
      "28     NO_QUERY    swimmingfishy   \n",
      "29     NO_QUERY       jerryfetus   \n",
      "...         ...              ...   \n",
      "9971   NO_QUERY     gracelkinney   \n",
      "9972   NO_QUERY      ktkeroscene   \n",
      "9973   NO_QUERY            RayYo   \n",
      "9974   NO_QUERY         _alover_   \n",
      "9975   NO_QUERY           yogilo   \n",
      "9976   NO_QUERY           shae75   \n",
      "9977   NO_QUERY     NiqueyAlston   \n",
      "9978   NO_QUERY      1ChoSenOne8   \n",
      "9979   NO_QUERY      IdaBergdahl   \n",
      "9980   NO_QUERY              snh   \n",
      "9981   NO_QUERY         lori1329   \n",
      "9982   NO_QUERY         Bthnycks   \n",
      "9983   NO_QUERY      Dianagonher   \n",
      "9984   NO_QUERY        MikeTreat   \n",
      "9985   NO_QUERY      andrewsmhay   \n",
      "9986   NO_QUERY       bofranklin   \n",
      "9987   NO_QUERY      jimwolffman   \n",
      "9988   NO_QUERY      pshhitscaty   \n",
      "9989   NO_QUERY        MegRaiano   \n",
      "9990   NO_QUERY             jmt1   \n",
      "9991   NO_QUERY          twitleb   \n",
      "9992   NO_QUERY      dennis_luis   \n",
      "9993   NO_QUERY        karebelle   \n",
      "9994   NO_QUERY   diegolikecrazy   \n",
      "9995   NO_QUERY    xxYOitsALEXxx   \n",
      "9996   NO_QUERY        whipzilla   \n",
      "9997   NO_QUERY            FFang   \n",
      "9998   NO_QUERY         JConnell   \n",
      "9999   NO_QUERY          bdottie   \n",
      "10000  NO_QUERY   RyanTrevMorris   \n",
      "\n",
      "                                                 Content  \n",
      "0      @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1      Picked Mich St to win it all from the get go. ...  \n",
      "2      throat is closing up and i had some string che...  \n",
      "3      If he doesn't get better in a few days, he cou...  \n",
      "4      @hillary006 I'm sure everyone has ruined my gi...  \n",
      "5                my little pinky finger hurts so much..   \n",
      "6               @alexbigman you left without saying hi!   \n",
      "7                        @hutsoncap  everything alright?  \n",
      "8      Yo jimo i cant talk on aim anymore, its glitch...  \n",
      "9      Didn't make it by here today.   They are sayin...  \n",
      "10                         @laurenlenewx awww i'm sorry   \n",
      "11     Goodnight everyone. Well I'm not feeling much ...  \n",
      "12     @KellyShibari i thought i saw you there! you w...  \n",
      "13     @mileycyrus I would too if it meant spending a...  \n",
      "14     The one day i really need to go into school an...  \n",
      "15          Stephen just left,  i miss him sooo much....  \n",
      "16     I'm having a panic attack, so I can't sleep. D...  \n",
      "17     @Heromancer come back to orlando again sooon! ...  \n",
      "18                                       Deadline ahead   \n",
      "19                                 @marissamonotony why   \n",
      "20     @Claire_S Will you be videoing or streaming or...  \n",
      "21     just been given ma marching orders, gotta go d...  \n",
      "22           Up since 3:00. Going to be a looooong day.   \n",
      "23                       i need something big to happen   \n",
      "24     @nikkiwoods Exactamundo!!! For some reason I t...  \n",
      "25     @edibow haha, I have snot power too......1 wee...  \n",
      "26     Going deaf in my right ear. Too many feedback ...  \n",
      "27     @Schofe Dunno who is there with you, but she's...  \n",
      "28     Going to school and enjoying my last day as a ...  \n",
      "29     I hate the V plotarc on True Blood  that poor ...  \n",
      "...                                                  ...  \n",
      "9971                          @dannygokey Good morning!   \n",
      "9972                          @dannykurily welcome home   \n",
      "9973   @jasonstoltzfus Ah gotcha-still cool! they're ...  \n",
      "9974   @Rachael90210 day's going great thanx  course ...  \n",
      "9975   @EliciaKoay you went overboard for the girl's ...  \n",
      "9976                   @Foxy_HotSawce good morning love   \n",
      "9977   @Amazing_stephy um uh duh dnt think i wld kno ...  \n",
      "9978   This Morning I woke up feeling like money, I j...  \n",
      "9979      @lighttrickphoto Coool, so u r from Finland ?   \n",
      "9980   @rachbarnhart I have plenty of opinions that I...  \n",
      "9981   @RecipeGirl My MIL has made that before. Oh my...  \n",
      "9982   @hobnobsftww_  oo paramore are bringing out a ...  \n",
      "9983                            @bigwormy good and you?   \n",
      "9984           @PreciousGemGem thanks! will investigate   \n",
      "9985   @leune I tried changing the &lt;x&gt;100&lt;/x...  \n",
      "9986   Christ, it's sticky today... Taking a stroll h...  \n",
      "9987   @colinhewitt Not yet..  Had to put together an...  \n",
      "9988   @Littlehotrod  yeah its just me ill be in play...  \n",
      "9989   @Larry_Keigwin I can't wait for your classes a...  \n",
      "9990            hey it's Jeff  http://aweber.com/b/1huHV  \n",
      "9991                @UxSoup Soup you already confirmed!   \n",
      "9992   @mikasounds if you like to support talented yo...  \n",
      "9993   realizing you were WRONG can be humbling. but ...  \n",
      "9994   @MariaLKanellis true, true, also gets you into...  \n",
      "9995   http://twitpic.com/7ham4 - i know now what is ...  \n",
      "9996   - had a great time with some of the best peopl...  \n",
      "9997   @Tyrese4ReaL Tyreseee, when you're heading to ...  \n",
      "9998   @theokk don't know what you could possibly mea...  \n",
      "9999           What a pretty day  &quot;Just smile&quot;  \n",
      "10000  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[10001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custome dataset class for twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        sample = {'content': content, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)\n",
    "\n",
    "# Load word embeddings from pretrained embeddings file en-cw.txt, courtesy CS224N\n",
    "# For final, use Word2Vec embeddings, but for now this should suffice\n",
    "word_vectors = {}\n",
    "for line in open('./data/en-cw.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token IDs from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize datasets with token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize(train_x_raw)\n",
    "\n",
    "dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "test_x = vectorize(test_x_raw)\n",
    "\n",
    "train_dataset = TweetDataset(train_x, train_y)\n",
    "dev_dataset = TweetDataset(dev_x, dev_y)\n",
    "test_dataset = TweetDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), 50)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class definition, courtesy https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embeddings.shape[1])) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.sigmoid(self.fc(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, embeddings, hidden_dim, n_layers, output_dim, bidirectional = True, dropout = 0.5):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "#         self.rnn = nn.LSTM(embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         #x = [sent len, batch size]\n",
    "        \n",
    "#         embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "#         #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "#         output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "#         #output = [sent len, batch size, hid dim * num directions]\n",
    "#         #hidden = [num layers * num directions, batch size, hid dim]\n",
    "#         #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "#         #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "#         #and apply dropout\n",
    "        \n",
    "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "#         #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "#         return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_dim = 256\n",
    "# output_dim = 1\n",
    "# n_layers = 2\n",
    "\n",
    "# RNN_model = RNN(embeddings_matrix, hidden_dim, n_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 100\n",
    "filter_sizes = [1, 2, 3, 4]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "CNN_model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and criterion (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy score, i.e. percentage correct per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        #print (\"Training on batch #\" + str(batchnum))\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        if train_x.shape[1] == 1: continue\n",
    "        #print (train_y.view(-1).shape)\n",
    "        \n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        #print (predictions.shape)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "# train(RNN_model, enumerate(train_loader), optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            #print (predictions)\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_loader), epoch_acc / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, dev_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 1.510 | Train Acc: 50.81% | Val. Loss: 1.521 | Val. Acc: 50.80% |\n",
      "| Epoch: 02 | Train Loss: 1.923 | Train Acc: 55.39% | Val. Loss: 1.647 | Val. Acc: 57.03% |\n",
      "| Epoch: 03 | Train Loss: 2.280 | Train Acc: 66.69% | Val. Loss: 5.376 | Val. Acc: 56.12% |\n",
      "| Epoch: 04 | Train Loss: 1.612 | Train Acc: 82.55% | Val. Loss: 6.247 | Val. Acc: 60.34% |\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(CNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(CNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "test_loss, test_acc = evaluate(CNN_model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN Model over 5 epochs (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# optimizer = optim.Adam(RNN_model.parameters())\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RNN_model = RNN_model.to(device)\n",
    "\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "# N_EPOCHS = 5 # Since the RNN is so slow\n",
    "\n",
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "    \n",
    "# dev_loader = DataLoader(dev_dataset,\n",
    "#                   batch_size=40,\n",
    "#                   shuffle=False,\n",
    "#                   num_workers=4\n",
    "#                  # pin_memory=True # CUDA only\n",
    "#                  )\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "\n",
    "#     train_loss, train_acc = train(RNN_model, train_loader, optimizer, criterion)\n",
    "#     valid_loss, valid_acc = evaluate(RNN_model, dev_loader, criterion)\n",
    "    \n",
    "#     print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
