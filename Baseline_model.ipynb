{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  Pos_Neg          ID                          Date  \\\n",
      "0               0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
      "1               1        0  1467815988  Mon Apr 06 22:21:09 PDT 2009   \n",
      "2               2        0  1467823851  Mon Apr 06 22:23:09 PDT 2009   \n",
      "3               3        0  1467836500  Mon Apr 06 22:26:28 PDT 2009   \n",
      "4               4        0  1467841832  Mon Apr 06 22:27:55 PDT 2009   \n",
      "5               5        0  1467853356  Mon Apr 06 22:30:54 PDT 2009   \n",
      "6               6        0  1467859820  Mon Apr 06 22:32:36 PDT 2009   \n",
      "7               7        0  1467871661  Mon Apr 06 22:35:41 PDT 2009   \n",
      "8               8        0  1467876652  Mon Apr 06 22:37:03 PDT 2009   \n",
      "9               9        0  1467882491  Mon Apr 06 22:38:37 PDT 2009   \n",
      "10             10        0  1467894600  Mon Apr 06 22:41:51 PDT 2009   \n",
      "11             11        0  1467899753  Mon Apr 06 22:43:18 PDT 2009   \n",
      "12             12        0  1467909222  Mon Apr 06 22:45:53 PDT 2009   \n",
      "13             13        0  1467917177  Mon Apr 06 22:48:08 PDT 2009   \n",
      "14             14        0  1467926632  Mon Apr 06 22:50:51 PDT 2009   \n",
      "15             15        0  1467932208  Mon Apr 06 22:52:25 PDT 2009   \n",
      "16             16        0  1467943526  Mon Apr 06 22:55:40 PDT 2009   \n",
      "17             17        0  1467949681  Mon Apr 06 22:57:27 PDT 2009   \n",
      "18             18        0  1467953733  Mon Apr 06 22:58:37 PDT 2009   \n",
      "19             19        0  1467965994  Mon Apr 06 23:01:56 PDT 2009   \n",
      "20             20        0  1467972262  Mon Apr 06 23:03:39 PDT 2009   \n",
      "21             21        0  1467982077  Mon Apr 06 23:06:29 PDT 2009   \n",
      "22             22        0  1467986889  Mon Apr 06 23:07:54 PDT 2009   \n",
      "23             23        0  1467995216  Mon Apr 06 23:10:17 PDT 2009   \n",
      "24             24        0  1468000948  Mon Apr 06 23:11:55 PDT 2009   \n",
      "25             25        0  1468007877  Mon Apr 06 23:13:57 PDT 2009   \n",
      "26             26        0  1468017259  Mon Apr 06 23:16:45 PDT 2009   \n",
      "27             27        0  1468023868  Mon Apr 06 23:18:48 PDT 2009   \n",
      "28             28        0  1468033983  Mon Apr 06 23:21:57 PDT 2009   \n",
      "29             29        0  1468038893  Mon Apr 06 23:23:33 PDT 2009   \n",
      "...           ...      ...         ...                           ...   \n",
      "49971       49971        4  2193344006  Tue Jun 16 08:19:48 PDT 2009   \n",
      "49972       49972        4  2193345339  Tue Jun 16 08:19:56 PDT 2009   \n",
      "49973       49973        4  2193346991  Tue Jun 16 08:20:03 PDT 2009   \n",
      "49974       49974        4  2193371250  Tue Jun 16 08:22:01 PDT 2009   \n",
      "49975       49975        4  2193373009  Tue Jun 16 08:22:10 PDT 2009   \n",
      "49976       49976        4  2193374881  Tue Jun 16 08:22:19 PDT 2009   \n",
      "49977       49977        4  2193402232  Tue Jun 16 08:24:36 PDT 2009   \n",
      "49978       49978        4  2193403814  Tue Jun 16 08:24:44 PDT 2009   \n",
      "49979       49979        4  2193404869  Tue Jun 16 08:24:49 PDT 2009   \n",
      "49980       49980        4  2193427329  Tue Jun 16 08:26:39 PDT 2009   \n",
      "49981       49981        4  2193428354  Tue Jun 16 08:26:45 PDT 2009   \n",
      "49982       49982        4  2193450427  Tue Jun 16 08:28:32 PDT 2009   \n",
      "49983       49983        4  2193452183  Tue Jun 16 08:28:41 PDT 2009   \n",
      "49984       49984        4  2193453445  Tue Jun 16 08:28:47 PDT 2009   \n",
      "49985       49985        4  2193454592  Tue Jun 16 08:28:53 PDT 2009   \n",
      "49986       49986        4  2193475408  Tue Jun 16 08:30:32 PDT 2009   \n",
      "49987       49987        4  2193476829  Tue Jun 16 08:30:39 PDT 2009   \n",
      "49988       49988        4  2193478813  Tue Jun 16 08:30:48 PDT 2009   \n",
      "49989       49989        4  2193502163  Tue Jun 16 08:32:41 PDT 2009   \n",
      "49990       49990        4  2193503480  Tue Jun 16 08:32:48 PDT 2009   \n",
      "49991       49991        4  2193525430  Tue Jun 16 08:34:35 PDT 2009   \n",
      "49992       49992        4  2193526931  Tue Jun 16 08:34:42 PDT 2009   \n",
      "49993       49993        4  2193528475  Tue Jun 16 08:34:50 PDT 2009   \n",
      "49994       49994        4  2193550768  Tue Jun 16 08:36:38 PDT 2009   \n",
      "49995       49995        4  2193552024  Tue Jun 16 08:36:44 PDT 2009   \n",
      "49996       49996        4  2193553464  Tue Jun 16 08:36:51 PDT 2009   \n",
      "49997       49997        4  2193575004  Tue Jun 16 08:38:38 PDT 2009   \n",
      "49998       49998        4  2193576573  Tue Jun 16 08:38:46 PDT 2009   \n",
      "49999       49999        4  2193578151  Tue Jun 16 08:38:54 PDT 2009   \n",
      "50000       50000        4  2193602129  Tue Jun 16 08:40:50 PDT 2009   \n",
      "\n",
      "          QUERY             User  \\\n",
      "0      NO_QUERY  _TheSpecialOne_   \n",
      "1      NO_QUERY         merisssa   \n",
      "2      NO_QUERY         ericg622   \n",
      "3      NO_QUERY   natalieantipas   \n",
      "4      NO_QUERY           bgoers   \n",
      "5      NO_QUERY         dbmendel   \n",
      "6      NO_QUERY    msbutt3rfly14   \n",
      "7      NO_QUERY         ciairuhh   \n",
      "8      NO_QUERY    SupernovaGirl   \n",
      "9      NO_QUERY   Stereo_Skyline   \n",
      "10     NO_QUERY           dreaaa   \n",
      "11     NO_QUERY       Sheezy3380   \n",
      "12     NO_QUERY           LAbite   \n",
      "13     NO_QUERY         nchokkan   \n",
      "14     NO_QUERY  kailashvasupati   \n",
      "15     NO_QUERY        rachelgab   \n",
      "16     NO_QUERY         javajive   \n",
      "17     NO_QUERY      harishanker   \n",
      "18     NO_QUERY         MonikkaB   \n",
      "19     NO_QUERY      lanaveenker   \n",
      "20     NO_QUERY    Smith_Cameron   \n",
      "21     NO_QUERY         mcdoogie   \n",
      "22     NO_QUERY         BrianG2k   \n",
      "23     NO_QUERY     Reynaga12345   \n",
      "24     NO_QUERY      Hanster7705   \n",
      "25     NO_QUERY           John2x   \n",
      "26     NO_QUERY          mishkia   \n",
      "27     NO_QUERY       ChrisMoody   \n",
      "28     NO_QUERY  patrick_ritchie   \n",
      "29     NO_QUERY   DangeresqueBen   \n",
      "...         ...              ...   \n",
      "49971  NO_QUERY      Amanda_Mosh   \n",
      "49972  NO_QUERY     spacegrace19   \n",
      "49973  NO_QUERY          flaka09   \n",
      "49974  NO_QUERY   adambuckeridge   \n",
      "49975  NO_QUERY    xxYOitsALEXxx   \n",
      "49976  NO_QUERY        MrsLoulou   \n",
      "49977  NO_QUERY        krystlerb   \n",
      "49978  NO_QUERY       lyddiechoi   \n",
      "49979  NO_QUERY       nut_cookie   \n",
      "49980  NO_QUERY        whipzilla   \n",
      "49981  NO_QUERY      Bloodl3tt3r   \n",
      "49982  NO_QUERY      HelloGracey   \n",
      "49983  NO_QUERY            WiiDS   \n",
      "49984  NO_QUERY   camilamariotto   \n",
      "49985  NO_QUERY            FFang   \n",
      "49986  NO_QUERY       SamNiley11   \n",
      "49987  NO_QUERY     Septimus1812   \n",
      "49988  NO_QUERY         davoloid   \n",
      "49989  NO_QUERY       JohnWeston   \n",
      "49990  NO_QUERY         JConnell   \n",
      "49991  NO_QUERY  MandeeLeigh1911   \n",
      "49992  NO_QUERY   Killer_Burrito   \n",
      "49993  NO_QUERY            Shash   \n",
      "49994  NO_QUERY        mlvlatina   \n",
      "49995  NO_QUERY          bdottie   \n",
      "49996  NO_QUERY     UrBaN_eLySsE   \n",
      "49997  NO_QUERY  say_my_name_too   \n",
      "49998  NO_QUERY            Emm94   \n",
      "49999  NO_QUERY  victoriaquinnxo   \n",
      "50000  NO_QUERY   RyanTrevMorris   \n",
      "\n",
      "                                                 Content  \n",
      "0      @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1      thought sleeping in was an option tomorrow but...  \n",
      "2      I had such a nice day. Too bad the rain comes ...  \n",
      "3      so rylee,grace...wana go steve's party or not?...  \n",
      "4                                           I'm so cold   \n",
      "5      Picked Mich St to win it all from the get go. ...  \n",
      "6                            spencer is not a good guy.   \n",
      "7      I miss you twitter. My phone broke, now I'm us...  \n",
      "8      @burgaw Ooooooh! *sealclap* See, I download sh...  \n",
      "9      @ThaStevieG but what I really want is my old b...  \n",
      "10     throat is closing up and i had some string che...  \n",
      "11                      New Testament Test at 9:30 am     \n",
      "12     Pepperoni rolls in L.A.?: I called Valentino's...  \n",
      "13     @KishoreK this is strange, illegal torrents av...  \n",
      "14     Dammit, episode 3 of Kings won't play for some...  \n",
      "15     If he doesn't get better in a few days, he cou...  \n",
      "16     @pratama Same iMac came out $320 more in Indon...  \n",
      "17                  @sasii I know exactly how you feel!   \n",
      "18     @paul_e_wog Wait...is it a game or just episod...  \n",
      "19     @AmandaEnglund Sorry to hear about your loss. ...  \n",
      "20     @hillary006 I'm sure everyone has ruined my gi...  \n",
      "21     Spring break is here at last, but no one is he...  \n",
      "22                         @lilbucknuts41 not an option   \n",
      "23                           Still doing my homework!!!   \n",
      "24                        i feel sick  too much icecream  \n",
      "25               my little pinky finger hurts so much..   \n",
      "26     aaaaand back to my literature review  At least...  \n",
      "27     One of the hardest thing with this schedule, n...  \n",
      "28                          @featherinair call me back.   \n",
      "29             @therealnph Twitter hates us both then.    \n",
      "...                                                  ...  \n",
      "49971  @kruss87 I know  I got it a few days ago haha ...  \n",
      "49972  helllllo twitter! #iremember playing #haveyoue...  \n",
      "49973  i miss aric i dont want to but i do , jimmy is...  \n",
      "49974  @spiralhosting Cool , I look forward to it  Lo...  \n",
      "49975  http://twitpic.com/7ham4 - i know now what is ...  \n",
      "49976              @phdinparenting It was a great post!   \n",
      "49977  @TANGG GT was a good movie...although I spent ...  \n",
      "49978  is off to lesson then ortho appt.  Dreading to...  \n",
      "49979              Getting offline! Take care everyone.   \n",
      "49980  - had a great time with some of the best peopl...  \n",
      "49981  Not a problem @rafaelladm  Thanks â« http://b...  \n",
      "49982  Exploring the world of Twitter   Listening to ...  \n",
      "49983  And PEGI Wins. The Fear Spider Will Be Staying...  \n",
      "49984         @alicebarrooss happy birthday, alice!!!!!   \n",
      "49985  @Tyrese4ReaL Tyreseee, when you're heading to ...  \n",
      "49986                                  @francii_ me too   \n",
      "49987  @radagast22 It's not the 'nillas I worry about...  \n",
      "49988  @theskink And in car?  On street via Mobile?  ...  \n",
      "49989          http://twitpic.com/7jomc - Party invites   \n",
      "49990  @theokk don't know what you could possibly mea...  \n",
      "49991  Watching pride and prejudice again &lt;3 then ...  \n",
      "49992  Okay so the plan is to go to hot topic and buy...  \n",
      "49993                        @linksforluv  you betcha!!   \n",
      "49994                   @asimkovsky thanks for the info   \n",
      "49995          What a pretty day  &quot;Just smile&quot;  \n",
      "49996  @FoSho174 LMAO @ capture the flag..bless their...  \n",
      "49997                 @say_my_name TRAITOR!!!! love you   \n",
      "49998  Is back from getting the new jonas brothers cd...  \n",
      "49999   done la examen! easy peasy  so proud of myself!!  \n",
      "50000  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[50001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custome dataset class for twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        sample = {'content': content, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)\n",
    "\n",
    "# Load word embeddings from pretrained embeddings file en-cw.txt, courtesy CS224N\n",
    "# For final, use Word2Vec embeddings, but for now this should suffice\n",
    "word_vectors = {}\n",
    "for line in open('./data/en-cw.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token IDs from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize datasets with token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize(train_x_raw)\n",
    "\n",
    "dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "test_x = vectorize(test_x_raw)\n",
    "\n",
    "train_dataset = TweetDataset(train_x, train_y)\n",
    "dev_dataset = TweetDataset(dev_x, dev_y)\n",
    "test_dataset = TweetDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), 50)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class definition, courtesy https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embeddings.shape[1])) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim, n_layers, output_dim, bidirectional = True, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.rnn = nn.LSTM(embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_dim = 256\n",
    "# output_dim = 1\n",
    "# n_layers = 2\n",
    "\n",
    "# RNN_model = RNN(embeddings_matrix, hidden_dim, n_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 200\n",
    "filter_sizes = [1, 2, 3, 4]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "CNN_model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and criterion (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy score, i.e. percentage correct per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        #print (\"Training on batch #\" + str(batchnum))\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        if train_x.shape[1] == 1: continue\n",
    "        #print (train_y.view(-1).shape)\n",
    "        \n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        #print (predictions.shape)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "# train(RNN_model, enumerate(train_loader), optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            #print (predictions)\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_loader), epoch_acc / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, dev_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 15.092 | Train Acc: 51.66% | Val. Loss: 31.181 | Val. Acc: 50.10% |\n",
      "| Epoch: 02 | Train Loss: 75.332 | Train Acc: 60.70% | Val. Loss: 112.320 | Val. Acc: 57.13% |\n",
      "| Epoch: 03 | Train Loss: 136.276 | Train Acc: 71.26% | Val. Loss: 260.590 | Val. Acc: 59.24% |\n",
      "| Epoch: 04 | Train Loss: 134.067 | Train Acc: 78.23% | Val. Loss: 742.478 | Val. Acc: 61.95% |\n",
      "| Epoch: 05 | Train Loss: 128.239 | Train Acc: 81.71% | Val. Loss: 2106.801 | Val. Acc: 59.64% |\n",
      "| Epoch: 06 | Train Loss: 151.424 | Train Acc: 83.67% | Val. Loss: 4782.700 | Val. Acc: 58.53% |\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=64,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(CNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(CNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "test_loss, test_acc = evaluate(CNN_model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN Model over 5 epochs (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(RNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "RNN_model = RNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 5 # Since the RNN is so slow\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=40,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  batch_size=40,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(RNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(RNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
