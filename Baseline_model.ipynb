{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  Pos_Neg          ID                          Date  \\\n",
      "0               0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
      "1               1        0  1467839737  Mon Apr 06 22:27:21 PDT 2009   \n",
      "2               2        0  1467873004  Mon Apr 06 22:36:03 PDT 2009   \n",
      "3               3        0  1467901188  Mon Apr 06 22:43:43 PDT 2009   \n",
      "4               4        0  1467932208  Mon Apr 06 22:52:25 PDT 2009   \n",
      "5               5        0  1467963715  Mon Apr 06 23:01:18 PDT 2009   \n",
      "6               6        0  1467992207  Mon Apr 06 23:09:26 PDT 2009   \n",
      "7               7        0  1468019453  Mon Apr 06 23:17:25 PDT 2009   \n",
      "8               8        0  1468047066  Mon Apr 06 23:26:06 PDT 2009   \n",
      "9               9        0  1468075671  Mon Apr 06 23:34:52 PDT 2009   \n",
      "10             10        0  1468106399  Mon Apr 06 23:44:57 PDT 2009   \n",
      "11             11        0  1468129897  Mon Apr 06 23:52:43 PDT 2009   \n",
      "12             12        0  1468155926  Tue Apr 07 00:01:16 PDT 2009   \n",
      "13             13        0  1468185887  Tue Apr 07 00:10:51 PDT 2009   \n",
      "14             14        0  1468209388  Tue Apr 07 00:18:42 PDT 2009   \n",
      "15             15        0  1468235599  Tue Apr 07 00:27:43 PDT 2009   \n",
      "16             16        0  1468262100  Tue Apr 07 00:36:46 PDT 2009   \n",
      "17             17        0  1468290857  Tue Apr 07 00:46:52 PDT 2009   \n",
      "18             18        0  1468319163  Tue Apr 07 00:57:02 PDT 2009   \n",
      "19             19        0  1468342622  Tue Apr 07 01:05:16 PDT 2009   \n",
      "20             20        0  1468371395  Tue Apr 07 01:15:42 PDT 2009   \n",
      "21             21        0  1468392589  Tue Apr 07 01:23:27 PDT 2009   \n",
      "22             22        0  1468419545  Tue Apr 07 01:33:19 PDT 2009   \n",
      "23             23        0  1468445455  Tue Apr 07 01:42:55 PDT 2009   \n",
      "24             24        0  1468470368  Tue Apr 07 01:52:04 PDT 2009   \n",
      "25             25        0  1468495130  Tue Apr 07 02:01:15 PDT 2009   \n",
      "26             26        0  1468522016  Tue Apr 07 02:10:57 PDT 2009   \n",
      "27             27        0  1468550790  Tue Apr 07 02:21:35 PDT 2009   \n",
      "28             28        0  1468575832  Tue Apr 07 02:30:32 PDT 2009   \n",
      "29             29        0  1468604810  Tue Apr 07 02:40:55 PDT 2009   \n",
      "...           ...      ...         ...                           ...   \n",
      "13305       13305        4  2192720251  Tue Jun 16 07:27:11 PDT 2009   \n",
      "13306       13306        4  2192745446  Tue Jun 16 07:29:25 PDT 2009   \n",
      "13307       13307        4  2192770352  Tue Jun 16 07:31:29 PDT 2009   \n",
      "13308       13308        4  2192811085  Tue Jun 16 07:35:01 PDT 2009   \n",
      "13309       13309        4  2192837781  Tue Jun 16 07:37:23 PDT 2009   \n",
      "13310       13310        4  2192861177  Tue Jun 16 07:39:27 PDT 2009   \n",
      "13311       13311        4  2192887529  Tue Jun 16 07:41:43 PDT 2009   \n",
      "13312       13312        4  2192935901  Tue Jun 16 07:45:55 PDT 2009   \n",
      "13313       13313        4  2192959502  Tue Jun 16 07:47:56 PDT 2009   \n",
      "13314       13314        4  2192984297  Tue Jun 16 07:50:05 PDT 2009   \n",
      "13315       13315        4  2193009495  Tue Jun 16 07:52:13 PDT 2009   \n",
      "13316       13316        4  2193053613  Tue Jun 16 07:55:59 PDT 2009   \n",
      "13317       13317        4  2193080471  Tue Jun 16 07:58:15 PDT 2009   \n",
      "13318       13318        4  2193106297  Tue Jun 16 08:00:21 PDT 2009   \n",
      "13319       13319        4  2193152774  Tue Jun 16 08:04:06 PDT 2009   \n",
      "13320       13320        4  2193179229  Tue Jun 16 08:06:14 PDT 2009   \n",
      "13321       13321        4  2193188976  Tue Jun 16 08:07:02 PDT 2009   \n",
      "13322       13322        4  2193223854  Tue Jun 16 08:09:55 PDT 2009   \n",
      "13323       13323        4  2193276683  Tue Jun 16 08:14:16 PDT 2009   \n",
      "13324       13324        4  2193304392  Tue Jun 16 08:16:31 PDT 2009   \n",
      "13325       13325        4  2193320231  Tue Jun 16 08:17:49 PDT 2009   \n",
      "13326       13326        4  2193346098  Tue Jun 16 08:19:59 PDT 2009   \n",
      "13327       13327        4  2193401077  Tue Jun 16 08:24:30 PDT 2009   \n",
      "13328       13328        4  2193427329  Tue Jun 16 08:26:39 PDT 2009   \n",
      "13329       13329        4  2193453166  Tue Jun 16 08:28:46 PDT 2009   \n",
      "13330       13330        4  2193477741  Tue Jun 16 08:30:43 PDT 2009   \n",
      "13331       13331        4  2193525714  Tue Jun 16 08:34:36 PDT 2009   \n",
      "13332       13332        4  2193552024  Tue Jun 16 08:36:44 PDT 2009   \n",
      "13333       13333        4  2193577726  Tue Jun 16 08:38:52 PDT 2009   \n",
      "13334       13334        4  2193602129  Tue Jun 16 08:40:50 PDT 2009   \n",
      "\n",
      "          QUERY             User  \\\n",
      "0      NO_QUERY  _TheSpecialOne_   \n",
      "1      NO_QUERY           JenBah   \n",
      "2      NO_QUERY         omgitsjo   \n",
      "3      NO_QUERY        bonerjamz   \n",
      "4      NO_QUERY        rachelgab   \n",
      "5      NO_QUERY      missmadison   \n",
      "6      NO_QUERY        Huddy1124   \n",
      "7      NO_QUERY          Pan_duh   \n",
      "8      NO_QUERY        dianapwns   \n",
      "9      NO_QUERY        im_mature   \n",
      "10     NO_QUERY     robotwarlord   \n",
      "11     NO_QUERY          AMEFACE   \n",
      "12     NO_QUERY          Jordyss   \n",
      "13     NO_QUERY     micahmarquis   \n",
      "14     NO_QUERY         khanhlnq   \n",
      "15     NO_QUERY    drxgirlfriend   \n",
      "16     NO_QUERY          Fatty_D   \n",
      "17     NO_QUERY  ACTinglikeamama   \n",
      "18     NO_QUERY           egg104   \n",
      "19     NO_QUERY     supervelerey   \n",
      "20     NO_QUERY       tlelover91   \n",
      "21     NO_QUERY       SarahSaner   \n",
      "22     NO_QUERY           hertog   \n",
      "23     NO_QUERY          sahmura   \n",
      "24     NO_QUERY          jj_tins   \n",
      "25     NO_QUERY         carakole   \n",
      "26     NO_QUERY        NusardelO   \n",
      "27     NO_QUERY        amygirl28   \n",
      "28     NO_QUERY   RedVelvetHeart   \n",
      "29     NO_QUERY    paperclipface   \n",
      "...         ...              ...   \n",
      "13305  NO_QUERY         SweetGio   \n",
      "13306  NO_QUERY     goodtobeglad   \n",
      "13307  NO_QUERY         monmen07   \n",
      "13308  NO_QUERY         lori1329   \n",
      "13309  NO_QUERY   CaityPineapple   \n",
      "13310  NO_QUERY       hildegunni   \n",
      "13311  NO_QUERY       TheFamulus   \n",
      "13312  NO_QUERY        MikeTreat   \n",
      "13313  NO_QUERY     BethanieChan   \n",
      "13314  NO_QUERY    StephanieLW08   \n",
      "13315  NO_QUERY     jeremiahalva   \n",
      "13316  NO_QUERY      jimwolffman   \n",
      "13317  NO_QUERY       harmony333   \n",
      "13318  NO_QUERY      pennymoore5   \n",
      "13319  NO_QUERY        ZyonSwope   \n",
      "13320  NO_QUERY             jmt1   \n",
      "13321  NO_QUERY           TC_DNB   \n",
      "13322  NO_QUERY        Steph2611   \n",
      "13323  NO_QUERY   LauraBarcelona   \n",
      "13324  NO_QUERY        karebelle   \n",
      "13325  NO_QUERY         SteveRal   \n",
      "13326  NO_QUERY        Placehold   \n",
      "13327  NO_QUERY       vicki_anne   \n",
      "13328  NO_QUERY        whipzilla   \n",
      "13329  NO_QUERY       ikeapencil   \n",
      "13330  NO_QUERY      just_chalie   \n",
      "13331  NO_QUERY  EmilyatMeritain   \n",
      "13332  NO_QUERY          bdottie   \n",
      "13333  NO_QUERY         FrayBaby   \n",
      "13334  NO_QUERY   RyanTrevMorris   \n",
      "\n",
      "                                                 Content  \n",
      "0      @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1      Is pissed off that there's no ASBA's for a rad...  \n",
      "2      I miss him.  Can't wait to celebrate the Tar H...  \n",
      "3      @cococourtney i was just listening to the swee...  \n",
      "4      If he doesn't get better in a few days, he cou...  \n",
      "5      @Born_4_Broadway Lost  and it was St. Ignacius...  \n",
      "6      I hate converting movies just to put em on my ...  \n",
      "7      @rootbeersoup Yeah. Too bad people like a cert...  \n",
      "8               @alexbigman you left without saying hi!   \n",
      "9                  @onlysweeter I don't know the dance.   \n",
      "10     The one day I have to go to school is the same...  \n",
      "11     had the worst dream abt some turd face i used ...  \n",
      "12     Didn't make it by here today.   They are sayin...  \n",
      "13     A few catering gigs, very cool, getting ready ...  \n",
      "14                    @ChauV I has so many things to do   \n",
      "15           @imperiusrex Brahbrah. Ugh. Bed in a hour.   \n",
      "16     @KellyShibari i thought i saw you there! you w...  \n",
      "17     @Cezzadwen I think that it's pretty standard w...  \n",
      "18     @jeffkang greeeeat but now i ate all my hard w...  \n",
      "19     is so 'jeles' argh.  sy pn mau jln sm kau jg. ...  \n",
      "20          Stephen just left,  i miss him sooo much....  \n",
      "21     @mikebreed Its all up to us Mike.  I understan...  \n",
      "22     @fabianv what kind of docs? and what are you u...  \n",
      "23     @kristenkreuk fiuhh, nice to get info from you...  \n",
      "24                                       Deadline ahead   \n",
      "25     @islandiva147 I sent u a tweet yesterday but I...  \n",
      "26     Was going to make a site updates twitter accou...  \n",
      "27                    has got a cold coming  how shite!!  \n",
      "28     just been given ma marching orders, gotta go d...  \n",
      "29                                 Wet hair in my eyes.   \n",
      "...                                                  ...  \n",
      "13305  @OfficialTL Oh my God!!!!  We love New Moon an...  \n",
      "13306  PS just texting me (twice!) to test my fortitu...  \n",
      "13307                 I'm so exciteeed, @ecksssy!  Haha!  \n",
      "13308  @RecipeGirl My MIL has made that before. Oh my...  \n",
      "13309                                  @Madayar Cheers.   \n",
      "13310               @lasgalen I do - only takes 40 mins   \n",
      "13311  @BumbleWard Could be. I never watched them. Mr...  \n",
      "13312          @PreciousGemGem thanks! will investigate   \n",
      "13313                        it's a beautiful day today   \n",
      "13314  @VivaMiGlam its good. funniest movie ive seen ...  \n",
      "13315     @bennyalvarado hey alchy  how's working going?  \n",
      "13316  @colinhewitt Not yet..  Had to put together an...  \n",
      "13317  I am a M I L F..dont you forget...wash it away...  \n",
      "13318  @MWiesner G-L-A-D you like em! They are my fav...  \n",
      "13319  Grocery shopping! Hopefully its not 500 dollar...  \n",
      "13320           hey it's Jeff  http://aweber.com/b/1huHV  \n",
      "13321  just got back from a little shop called Electr...  \n",
      "13322              @bwechols getting paid is a problem?   \n",
      "13323    @ashleytisdale niceeeee!  we miss you in spain!  \n",
      "13324  realizing you were WRONG can be humbling. but ...  \n",
      "13325  getting the movie crybaby for someone that wan...  \n",
      "13326  @JalokimGraphics lmao yeah it sounds seriously...  \n",
      "13327                   is going to lay out by the pool   \n",
      "13328  - had a great time with some of the best peopl...  \n",
      "13329           Reading avalon high! Hah, I'm surprised   \n",
      "13330  @Elle_333 @jenfafer midnight showing peoples, ...  \n",
      "13331                        @alwaysfurst See you there   \n",
      "13332          What a pretty day  &quot;Just smile&quot;  \n",
      "13333     @pokapolas love the donut and the toadstool.    \n",
      "13334  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[13335 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custome dataset class for twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        sample = {'content': content, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/en-cw.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-75e68a8f846f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# For final, use Word2Vec embeddings, but for now this should suffice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/en-cw.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/en-cw.txt'"
     ]
    }
   ],
   "source": [
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)\n",
    "\n",
    "# Load word embeddings from pretrained embeddings file en-cw.txt, courtesy CS224N\n",
    "# For final, use Word2Vec embeddings, but for now this should suffice\n",
    "word_vectors = {}\n",
    "for line in open('./data/en-cw.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load character embeddings from pretrained embeddings file char-embeddings.txt, courtesy github user minimaxir\n",
    "char_vectors = {}\n",
    "for line in open('./data/char-embeddings.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    if len(sp) == 0: continue\n",
    "    char_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    \n",
    "print (char_vectors)\n",
    "print (len(char_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token IDs from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize datasets with token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = vectorize(train_x_raw)\n",
    "\n",
    "# dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "# test_x = vectorize(test_x_raw)\n",
    "\n",
    "# TEXT = data.Field(tokenize='spacy')\n",
    "# LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "train_dataset = TweetDataset(train_x_raw, train_y)\n",
    "dev_dataset = TweetDataset(dev_x_raw, dev_y)\n",
    "test_dataset = TweetDataset(test_x_raw, test_y)\n",
    "\n",
    "# TEXT.build_vocab(train_dataset, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "# LABEL.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), 50)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class definition, courtesy https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embeddings.shape[1])) for fs in filter_sizes])\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            torch.nn.init.xavier_uniform_(conv.weight)\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return (self.fc(cat))\n",
    "#         return self.sigmoid(self.fc(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, embeddings, hidden_dim, n_layers, output_dim, bidirectional = True, dropout = 0.5):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "#         self.rnn = nn.LSTM(embeddings.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         #x = [sent len, batch size]\n",
    "        \n",
    "#         embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "#         #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "#         output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "#         #output = [sent len, batch size, hid dim * num directions]\n",
    "#         #hidden = [num layers * num directions, batch size, hid dim]\n",
    "#         #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "#         #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "#         #and apply dropout\n",
    "        \n",
    "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "#         #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "#         return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_dim = 256\n",
    "# output_dim = 1\n",
    "# n_layers = 2\n",
    "\n",
    "# RNN_model = RNN(embeddings_matrix, hidden_dim, n_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 100\n",
    "filter_sizes = [1, 2, 3, 4]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "CNN_model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and criterion (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy score, i.e. percentage correct per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    return f1_score(y, rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        #print (\"Training on batch #\" + str(batchnum))\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x.shape)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        if train_x.shape[1] == 1: continue\n",
    "        #print (train_y.view(-1).shape)\n",
    "\n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        #print (predictions.shape)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        f1 = F1(predictions, train_y)\n",
    "        print (f1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "# train(RNN_model, enumerate(train_loader), optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            print (torch.round(predictions))\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_loader), epoch_acc / len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, dev_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(CNN_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  batch_size=128,\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(CNN_model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(CNN_model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "test_loss, test_acc = evaluate(CNN_model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN Model over 5 epochs (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# optimizer = optim.Adam(RNN_model.parameters())\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RNN_model = RNN_model.to(device)\n",
    "\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "# N_EPOCHS = 5 # Since the RNN is so slow\n",
    "\n",
    "# train_loader = DataLoader(train_dataset,\n",
    "#                       batch_size=40,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=4\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "    \n",
    "# dev_loader = DataLoader(dev_dataset,\n",
    "#                   batch_size=40,\n",
    "#                   shuffle=False,\n",
    "#                   num_workers=4\n",
    "#                  # pin_memory=True # CUDA only\n",
    "#                  )\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "\n",
    "#     train_loss, train_acc = train(RNN_model, train_loader, optimizer, criterion)\n",
    "#     valid_loss, valid_acc = evaluate(RNN_model, dev_loader, criterion)\n",
    "    \n",
    "#     print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
