{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "SEED = 1234\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "TWEET_LEN = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATA_FOLDER = '~/Local Documents/CS230/Project/Twitter-Sentiment/data/Data-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Pos_Neg          ID                          Date     QUERY  \\\n",
      "0              0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1              1        0  1467894600  Mon Apr 06 22:41:51 PDT 2009  NO_QUERY   \n",
      "2              2        0  1467972262  Mon Apr 06 23:03:39 PDT 2009  NO_QUERY   \n",
      "3              3        0  1468047066  Mon Apr 06 23:26:06 PDT 2009  NO_QUERY   \n",
      "4              4        0  1468121466  Mon Apr 06 23:49:56 PDT 2009  NO_QUERY   \n",
      "5              5        0  1468192521  Tue Apr 07 00:13:02 PDT 2009  NO_QUERY   \n",
      "6              6        0  1468262100  Tue Apr 07 00:36:46 PDT 2009  NO_QUERY   \n",
      "7              7        0  1468336500  Tue Apr 07 01:03:03 PDT 2009  NO_QUERY   \n",
      "8              8        0  1468401268  Tue Apr 07 01:26:36 PDT 2009  NO_QUERY   \n",
      "9              9        0  1468470368  Tue Apr 07 01:52:04 PDT 2009  NO_QUERY   \n",
      "10            10        0  1468543053  Tue Apr 07 02:18:49 PDT 2009  NO_QUERY   \n",
      "11            11        0  1468617899  Tue Apr 07 02:45:41 PDT 2009  NO_QUERY   \n",
      "12            12        0  1468686594  Tue Apr 07 03:09:58 PDT 2009  NO_QUERY   \n",
      "13            13        0  1468755779  Tue Apr 07 03:33:42 PDT 2009  NO_QUERY   \n",
      "14            14        0  1468833927  Tue Apr 07 03:58:41 PDT 2009  NO_QUERY   \n",
      "15            15        0  1468913518  Tue Apr 07 04:22:39 PDT 2009  NO_QUERY   \n",
      "16            16        0  1468998615  Tue Apr 07 04:46:29 PDT 2009  NO_QUERY   \n",
      "17            17        0  1469085298  Tue Apr 07 05:08:30 PDT 2009  NO_QUERY   \n",
      "18            18        0  1469176829  Tue Apr 07 05:29:55 PDT 2009  NO_QUERY   \n",
      "19            19        0  1469289352  Tue Apr 07 05:54:00 PDT 2009  NO_QUERY   \n",
      "20            20        0  1469414844  Tue Apr 07 06:18:34 PDT 2009  NO_QUERY   \n",
      "21            21        0  1469529446  Tue Apr 07 06:39:30 PDT 2009  NO_QUERY   \n",
      "22            22        0  1469663416  Tue Apr 07 07:03:12 PDT 2009  NO_QUERY   \n",
      "23            23        0  1469784713  Tue Apr 07 07:24:42 PDT 2009  NO_QUERY   \n",
      "24            24        0  1469901491  Tue Apr 07 07:45:24 PDT 2009  NO_QUERY   \n",
      "25            25        0  1470036251  Tue Apr 07 08:09:22 PDT 2009  NO_QUERY   \n",
      "26            26        0  1470152257  Tue Apr 07 08:30:14 PDT 2009  NO_QUERY   \n",
      "27            27        0  1548286554  Fri Apr 17 20:32:27 PDT 2009  NO_QUERY   \n",
      "28            28        0  1548551581  Fri Apr 17 21:16:15 PDT 2009  NO_QUERY   \n",
      "29            29        0  1548688822  Fri Apr 17 21:40:15 PDT 2009  NO_QUERY   \n",
      "...          ...      ...         ...                           ...       ...   \n",
      "4971        4971        4  2191449563  Tue Jun 16 05:17:59 PDT 2009  NO_QUERY   \n",
      "4972        4972        4  2191498786  Tue Jun 16 05:24:11 PDT 2009  NO_QUERY   \n",
      "4973        4973        4  2191549437  Tue Jun 16 05:30:23 PDT 2009  NO_QUERY   \n",
      "4974        4974        4  2191600931  Tue Jun 16 05:36:37 PDT 2009  NO_QUERY   \n",
      "4975        4975        4  2191665733  Tue Jun 16 05:44:17 PDT 2009  NO_QUERY   \n",
      "4976        4976        4  2191715430  Tue Jun 16 05:50:08 PDT 2009  NO_QUERY   \n",
      "4977        4977        4  2191787040  Tue Jun 16 05:58:15 PDT 2009  NO_QUERY   \n",
      "4978        4978        4  2191845446  Tue Jun 16 06:04:23 PDT 2009  NO_QUERY   \n",
      "4979        4979        4  2191904422  Tue Jun 16 06:10:30 PDT 2009  NO_QUERY   \n",
      "4980        4980        4  2191981070  Tue Jun 16 06:18:26 PDT 2009  NO_QUERY   \n",
      "4981        4981        4  2192041073  Tue Jun 16 06:24:35 PDT 2009  NO_QUERY   \n",
      "4982        4982        4  2192103859  Tue Jun 16 06:30:51 PDT 2009  NO_QUERY   \n",
      "4983        4983        4  2192182835  Tue Jun 16 06:38:37 PDT 2009  NO_QUERY   \n",
      "4984        4984        4  2192249130  Tue Jun 16 06:44:53 PDT 2009  NO_QUERY   \n",
      "4985        4985        4  2192315621  Tue Jun 16 06:51:01 PDT 2009  NO_QUERY   \n",
      "4986        4986        4  2192397932  Tue Jun 16 06:58:47 PDT 2009  NO_QUERY   \n",
      "4987        4987        4  2192514568  Tue Jun 16 07:09:03 PDT 2009  NO_QUERY   \n",
      "4988        4988        4  2192584392  Tue Jun 16 07:15:13 PDT 2009  NO_QUERY   \n",
      "4989        4989        4  2192673182  Tue Jun 16 07:23:05 PDT 2009  NO_QUERY   \n",
      "4990        4990        4  2192768974  Tue Jun 16 07:31:22 PDT 2009  NO_QUERY   \n",
      "4991        4991        4  2192839605  Tue Jun 16 07:37:33 PDT 2009  NO_QUERY   \n",
      "4992        4992        4  2192935901  Tue Jun 16 07:45:55 PDT 2009  NO_QUERY   \n",
      "4993        4993        4  2193007920  Tue Jun 16 07:52:05 PDT 2009  NO_QUERY   \n",
      "4994        4994        4  2193082128  Tue Jun 16 07:58:23 PDT 2009  NO_QUERY   \n",
      "4995        4995        4  2193179229  Tue Jun 16 08:06:14 PDT 2009  NO_QUERY   \n",
      "4996        4996        4  2193254134  Tue Jun 16 08:12:22 PDT 2009  NO_QUERY   \n",
      "4997        4997        4  2193322665  Tue Jun 16 08:18:02 PDT 2009  NO_QUERY   \n",
      "4998        4998        4  2193427329  Tue Jun 16 08:26:39 PDT 2009  NO_QUERY   \n",
      "4999        4999        4  2193503480  Tue Jun 16 08:32:48 PDT 2009  NO_QUERY   \n",
      "5000        5000        4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                 User                                            Content  \n",
      "0     _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1              dreaaa  throat is closing up and i had some string che...  \n",
      "2       Smith_Cameron  @hillary006 I'm sure everyone has ruined my gi...  \n",
      "3           dianapwns           @alexbigman you left without saying hi!   \n",
      "4         tominlumban  Yo jimo i cant talk on aim anymore, its glitch...  \n",
      "5      catherinestack                      @laurenlenewx awww i'm sorry   \n",
      "6             Fatty_D  @KellyShibari i thought i saw you there! you w...  \n",
      "7          bethasaurr  The one day i really need to go into school an...  \n",
      "8     soapdishsailing  I'm having a panic attack, so I can't sleep. D...  \n",
      "9             jj_tins                                    Deadline ahead   \n",
      "10           alunjohn  @Claire_S Will you be videoing or streaming or...  \n",
      "11        jaredgunter        Up since 3:00. Going to be a looooong day.   \n",
      "12      DivasMistress  @nikkiwoods Exactamundo!!! For some reason I t...  \n",
      "13          rpecknold  Going deaf in my right ear. Too many feedback ...  \n",
      "14      swimmingfishy  Going to school and enjoying my last day as a ...  \n",
      "15        Roonaldo107              @Tracy_R Evil!! I have a prawn salad   \n",
      "16          Poppypaws                         wishes things were easier   \n",
      "17      morganbrennan               I feel like DEATH.  My throat hurts.  \n",
      "18          tongirl02  House completly surprised me last night. I can...  \n",
      "19           Ciridian  would have liked to have gotten more sleep las...  \n",
      "20              skoop  I still dearly miss the symfony dev environmen...  \n",
      "21     IPanicAtDiscos   dentist later, I'm hoping it's my teeth plate...  \n",
      "22           amysav83                        @jeayese coz its cold rain   \n",
      "23       steffy_weffy                            @dinadb where are you?   \n",
      "24         auntyadele           @vix7 I swear I'm going to unfollow you   \n",
      "25          lindsaycb  @enoch111 Whoops. I got a little too happy. Do...  \n",
      "26             BenZee  @ericahoff  I'm sorry to hear that Erica... bu...  \n",
      "27    Flyinwatermelon  @lillyputian whats gotten into me these few da...  \n",
      "28         photokitty  Freakin' crap! I just bit my tongue on accident.   \n",
      "29        ubringmejoi                                   my nose is cold   \n",
      "...               ...                                                ...  \n",
      "4971         Aaronage  @williamtm &gt; treat us well over the years s...  \n",
      "4972          swizzem   uploadin my jonas pics  ill put some on in a bit  \n",
      "4973       Taoteaking  @sticksngiggles Perhaps your right ... but ......  \n",
      "4974            BoL7z  @Ninja_Catfish my canalphones went through the...  \n",
      "4975       Kawaii_Emi  @liveeisavampire http://twitpic.com/7iq0a - ni...  \n",
      "4976    FitnessFoodie  it's a beautiful morning and I had a creamy gl...  \n",
      "4977          Jenidvm  And if he thinks that cooing sweetly at me wil...  \n",
      "4978       AlyxxDione  Oh yeaa.... #squarespace i love you... forever...  \n",
      "4979    vandawilliams  is excited to make a new video. its been a while   \n",
      "4980        Clyde_DTH          make beatiful pics with my new Nikon D90   \n",
      "4981     minhtrangcat                 KEB - the very best bank in Korea   \n",
      "4982         xxbonnie  why am i up so early? well, at least my mommy ...  \n",
      "4983       thepaulkim  drinking blueberry green tea   oh and for thos...  \n",
      "4984       cindergela        @SaintJboyd hoy! i greeted you last night!   \n",
      "4985     pooping24998  Are you looking for a good value affiliate pro...  \n",
      "4986      ktkeroscene                         @dannykurily welcome home   \n",
      "4987         _alover_  @Rachael90210 day's going great thanx  course ...  \n",
      "4988           shae75                  @Foxy_HotSawce good morning love   \n",
      "4989      1ChoSenOne8  This Morning I woke up feeling like money, I j...  \n",
      "4990              snh  @rachbarnhart I have plenty of opinions that I...  \n",
      "4991         Bthnycks  @hobnobsftww_  oo paramore are bringing out a ...  \n",
      "4992        MikeTreat          @PreciousGemGem thanks! will investigate   \n",
      "4993       bofranklin  Christ, it's sticky today... Taking a stroll h...  \n",
      "4994      pshhitscaty  @Littlehotrod  yeah its just me ill be in play...  \n",
      "4995             jmt1           hey it's Jeff  http://aweber.com/b/1huHV  \n",
      "4996      dennis_luis  @mikasounds if you like to support talented yo...  \n",
      "4997   diegolikecrazy  @MariaLKanellis true, true, also gets you into...  \n",
      "4998        whipzilla  - had a great time with some of the best peopl...  \n",
      "4999         JConnell  @theokk don't know what you could possibly mea...  \n",
      "5000   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[5001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_FOLDER + 'train_mini.csv', encoding = 'latin-1')\n",
    "train_n = train_data.shape[0]\n",
    "print (train_data)\n",
    "\n",
    "dev_data = pd.read_csv(DATA_FOLDER + 'dev_mini.csv', encoding = 'latin-1')\n",
    "dev_n = dev_data.shape[0]\n",
    "#print (dev_data)\n",
    "\n",
    "test_data = pd.read_csv(DATA_FOLDER + 'test_mini.csv', encoding = 'latin-1')\n",
    "test_n = test_data.shape[0]\n",
    "#print (test_data)\n",
    "\n",
    "dataset = pd.concat([train_data, dev_data, test_data])\n",
    "dataset_n = dataset.shape[0]\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custome dataset class for twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        sample = {'content': content, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth x and y values\n",
    "train_x_raw = train_data.loc[:][\"Content\"]\n",
    "train_y = [0.0 if y == 0 else 1.0 for y in train_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (train_y)\n",
    "\n",
    "dev_x_raw = dev_data.loc[:][\"Content\"]\n",
    "dev_y = [0.0 if y == 0 else 1.0 for y in dev_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (dev_y)\n",
    "\n",
    "test_x_raw = test_data.loc[:][\"Content\"]\n",
    "test_y = [0.0 if y == 0 else 1.0 for y in test_data.loc[:][\"Pos_Neg\"]]\n",
    "#print (test_y)\n",
    "\n",
    "# Load word embeddings from pretrained embeddings file en-cw.txt, courtesy CS224N\n",
    "# For final, use Word2Vec embeddings, but for now this should suffice\n",
    "word_vectors = {}\n",
    "for line in open('./data/en-cw.txt').readlines():\n",
    "    sp = line.strip().split()\n",
    "    word_vectors[sp[0]] = [float(x) for x in sp[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token IDs from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok2id = {}\n",
    "\n",
    "for ex in dataset['Content']:\n",
    "    for w in word_tokenize(ex):\n",
    "        if w in string.punctuation:\n",
    "            continue\n",
    "        if not w in tok2id:\n",
    "            tok2id[w] = len(tok2id)\n",
    "\n",
    "tok2id[UNK] = len(tok2id)\n",
    "tok2id[PAD] = len(tok2id)\n",
    "# print (tok2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize datasets with token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples):\n",
    "    vec_examples = []\n",
    "    for ex in examples:\n",
    "        #print (ex)\n",
    "        sentence = []\n",
    "        for w in word_tokenize(ex):\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            if w in tok2id:\n",
    "                sentence.append(tok2id[w])\n",
    "        if len(sentence) < TWEET_LEN:\n",
    "            sentence += [tok2id[PAD] for i in range(TWEET_LEN - len(sentence))]\n",
    "        else:\n",
    "            sentence = sentence[:TWEET_LEN]\n",
    "        vec_examples.append(sentence)\n",
    "    return vec_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = vectorize(train_x_raw)\n",
    "\n",
    "dev_x = vectorize(dev_x_raw)\n",
    "\n",
    "test_x = vectorize(test_x_raw)\n",
    "\n",
    "train_dataset = TweetDataset(train_x, train_y)\n",
    "dev_dataset = TweetDataset(dev_x, dev_y)\n",
    "test_dataset = TweetDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (len(tok2id), 50)), dtype='float32')\n",
    "\n",
    "for token in tok2id:\n",
    "    i = tok2id[token]\n",
    "    if token in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token]\n",
    "    elif token.lower() in word_vectors:\n",
    "        embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "#print (embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class definition, courtesy https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embeddings.shape[0], embeddings.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embeddings.shape[1])) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        fc = self.fc(cat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 100\n",
    "filter_sizes = [3,4,5]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "model = CNN(embeddings_matrix, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and criterion (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy score, i.e. percentage correct per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batchnum, batch in enumerate(train_loader):\n",
    "        train_x = torch.stack(batch['content'])\n",
    "        #print (train_x)\n",
    "        train_y = batch['label'].float()\n",
    "        #train_y = batch['label'].long()\n",
    "        #print (train_x.shape)\n",
    "        #print (train_y.view(-1).shape)\n",
    "        \n",
    "        predictions = model.forward(train_x).squeeze(1)\n",
    "        # print (logits)\n",
    "        loss = criterion(predictions, train_y)\n",
    "        # print (loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions, train_y)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / (batchnum + 1), epoch_acc / (batchnum + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(model, train_dataset, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batchnum, batch in enumerate(dev_loader):\n",
    "            #print (batch)\n",
    "            dev_x = torch.stack(batch['content'])\n",
    "            #print (train_x)\n",
    "            dev_y = batch['label'].float()\n",
    "            \n",
    "            \n",
    "            predictions = model(dev_x).squeeze(1)\n",
    "            #print (predictions)\n",
    "            \n",
    "            loss = criterion(predictions, dev_y)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, dev_y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / (batchnum + 1), epoch_acc / (batchnum + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, dev_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.694 | Train Acc: 50.00% | Val. Loss: 0.693 | Val. Acc: 49.90% |\n",
      "| Epoch: 02 | Train Loss: 0.693 | Train Acc: 49.60% | Val. Loss: 0.693 | Val. Acc: 49.90% |\n",
      "| Epoch: 03 | Train Loss: 0.693 | Train Acc: 50.38% | Val. Loss: 0.693 | Val. Acc: 49.90% |\n",
      "| Epoch: 04 | Train Loss: 0.693 | Train Acc: 50.38% | Val. Loss: 0.693 | Val. Acc: 49.90% |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                      batch_size=40,\n",
    "                      shuffle=True,\n",
    "                      num_workers=4\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "    \n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                  batch_size=len(dev_dataset),\n",
    "                  shuffle=False,\n",
    "                  num_workers=4\n",
    "                 # pin_memory=True # CUDA only\n",
    "                 )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, dev_loader, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
